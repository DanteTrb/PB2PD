{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il dataset\n",
    "df = pd.read_excel(\"../data/raw/final_data_mondinoxicot.xlsx\")  # Adatta path se necessario\n",
    "\n",
    "# Calcola % NaN per colonna\n",
    "nan_percent = df.isna().mean().sort_values(ascending=False) * 100\n",
    "nan_df = nan_percent[nan_percent > 0].to_frame(name=\"Percentuale NaN\")\n",
    "nan_df.head(20)  # Mostra le 20 peggiori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Variabili chiave da tracciare per missingness\n",
    "vars_to_flag = [\n",
    "    \"Updrs-III\", \"Duration (years)\", \"AgeOnset\", \"OnsetCategory\",\n",
    "    \"DoubleSupport\", \"SingleSupport\", \"GaitSpeed\", \"StrideLength\"\n",
    "]\n",
    "\n",
    "# ðŸ”¹ Crea colonne binarie che indicano se il valore era mancante\n",
    "for col in vars_to_flag:\n",
    "    if col in df.columns:\n",
    "        safe_colname = col.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        df[f\"{safe_colname}_was_missing\"] = df[col].isna().astype(int)\n",
    "\n",
    "# ðŸ”¹ Controllo rapido\n",
    "df[[c for c in df.columns if \"_was_missing\" in c]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonne flag create\n",
    "flag_cols = [c for c in df.columns if c.endswith(\"_was_missing\")]\n",
    "\n",
    "# Quanti flag per soggetto\n",
    "df[\"missing_flags_sum\"] = df[flag_cols].sum(axis=1)\n",
    "\n",
    "# Distribuzione globale\n",
    "print(\"Distribuzione numero di flag per soggetto:\")\n",
    "print(df[\"missing_flags_sum\"].value_counts().sort_index())\n",
    "\n",
    "# Distribuzione per classe (se target_bin presente)\n",
    "if \"target_bin\" in df.columns:\n",
    "    print(\"\\nDistribuzione flag per classe:\")\n",
    "    print(df.groupby(\"target_bin\")[\"missing_flags_sum\"].describe().round(2))\n",
    "\n",
    "# Quanti hanno almeno 1 flag\n",
    "n_any = (df[\"missing_flags_sum\"] > 0).sum()\n",
    "print(f\"\\nSoggetti con â‰¥1 flag: {n_any} su {len(df)} \"\n",
    "      f\"({n_any/len(df)*100:.1f}%)\")\n",
    "\n",
    "# (Facoltativo) Esporta un report veloce\n",
    "df[[\"missing_flags_sum\"] + flag_cols].to_csv(\"../tables/missing_flags_report.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Colonne numeriche (escludi i flag *_was_missing)\n",
    "num_cols_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols_all if not c.endswith(\"_was_missing\")]\n",
    "\n",
    "# Colonne categoriche (object)\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Imputazione numeriche -> mediana (robusta)\n",
    "if len(num_cols) > 0:\n",
    "    imputer_num = SimpleImputer(strategy=\"median\")\n",
    "    df[num_cols] = imputer_num.fit_transform(df[num_cols])\n",
    "\n",
    "# Imputazione categoriche -> moda\n",
    "if len(cat_cols) > 0:\n",
    "    imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "    df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])\n",
    "\n",
    "# (Facoltativo: controllo rapido)\n",
    "print(f\"Imputate numeriche: {len(num_cols)} | categoriche: {len(cat_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 1) Ordinal encoding per l'esordio (early < middle < late)\n",
    "if \"OnsetCategory\" in df.columns:\n",
    "    oe = OrdinalEncoder(categories=[['1', '2', '3']])  # se giÃ  stringhe\n",
    "    # Coerciamo a stringhe per sicurezza\n",
    "    df[\"OnsetCategory\"] = df[\"OnsetCategory\"].astype(str)\n",
    "    df[[\"OnsetCategory\"]] = oe.fit_transform(df[[\"OnsetCategory\"]])\n",
    "\n",
    "# 2) One-hot per categoriche a bassa cardinalitÃ  (â‰¤4 livelli)\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "low_card_cat = [col for col in cat_cols if df[col].nunique() <= 4]\n",
    "\n",
    "if len(low_card_cat) > 0:\n",
    "    df = pd.get_dummies(df, columns=low_card_cat, drop_first=True)\n",
    "\n",
    "# 3) Drop variabili identificative e potenziale leakage\n",
    "cols_to_drop = [\"ID\", \"Surname\", \"Name\", \"ProdromalCount\", \"Center\"]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# Se dopo i dummies fosse comparso qualcosa tipo \"Center_<sito>\", rimuovilo\n",
    "center_dummy_cols = [c for c in df.columns if c.startswith(\"Center_\")]\n",
    "if center_dummy_cols:\n",
    "    df = df.drop(columns=center_dummy_cols, errors=\"ignore\")\n",
    "\n",
    "# 4) Controlli\n",
    "print(\"âœ… Shape finale:\", df.shape)\n",
    "print(\"ðŸ§¼ Tipi di dato post-cleaning:\\n\", df.dtypes.value_counts())\n",
    "print(\"ðŸ” Colonne non numeriche residue:\", df.select_dtypes(exclude=[np.number, 'bool']).columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Funzione robusta per coercizione numerica ---\n",
    "def to_numeric_safe(s: pd.Series):\n",
    "    s = s.astype(str).str.replace(r\"\\s+(?=\\d)\", \"\", regex=True)  # es. \"6 2\" -> \"62\"\n",
    "    s = s.str.replace(\",\", \".\", regex=False)                     # virgola decimale -> punto\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# --- 1) Evaluation Date -> datetime, poi rimuoviamo per evitare leakage ---\n",
    "if \"Evaluation Date\" in df.columns:\n",
    "    df[\"Evaluation_Date\"] = pd.to_datetime(df[\"Evaluation Date\"], errors=\"coerce\")\n",
    "    df = df.drop(columns=[\"Evaluation Date\"], errors=\"ignore\")\n",
    "# rimuovi anche la versione datetime dai dati per i modelli\n",
    "df = df.drop(columns=[\"Evaluation_Date\"], errors=\"ignore\")\n",
    "\n",
    "# --- 2) Age Onset -> numerico + rename coerente ad 'AgeOnset' ---\n",
    "if \"Age Onset\" in df.columns:\n",
    "    df[\"AgeOnset\"] = to_numeric_safe(df[\"Age Onset\"])\n",
    "    df = df.drop(columns=[\"Age Onset\"], errors=\"ignore\")\n",
    "\n",
    "# --- 3) LEDD e Updrs-III -> numerici robusti ---\n",
    "for col in [\"LEDD\", \"Updrs-III\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = to_numeric_safe(df[col])\n",
    "\n",
    "# --- Controllo finale ---\n",
    "print(\"ðŸ§¼ Tipi di dato attuali:\\n\", df.dtypes.value_counts())\n",
    "non_num = df.select_dtypes(exclude=[np.number, 'bool', 'datetime64[ns]']).columns.tolist()\n",
    "print(\"ðŸ” Colonne non numeriche residue:\", non_num if len(non_num) > 0 else \"Nessuna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“ Shape finale del dataset:\", df.shape)\n",
    "\n",
    "# 1. Missing values\n",
    "print(\"\\nðŸ”Ž Valori NaN residui:\")\n",
    "nan_residui = df.isna().sum()[df.isna().sum() > 0]\n",
    "if nan_residui.empty:\n",
    "    print(\"âœ… Nessun NaN residuo\")\n",
    "else:\n",
    "    print(nan_residui)\n",
    "\n",
    "# 2. Tipi di dato\n",
    "print(\"\\nðŸ“Š Tipi di dato:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# 3. Colonne non numeriche\n",
    "non_numeric_cols = df.select_dtypes(exclude=[\"int64\", \"float64\"]).columns\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(\"\\nâš ï¸ Colonne non numeriche:\", non_numeric_cols.tolist())\n",
    "else:\n",
    "    print(\"\\nâœ… Tutte le colonne sono numeriche\")\n",
    "\n",
    "# 4. Colonne con varianza nulla\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "constant_filter.fit(df)\n",
    "zero_var_cols = df.columns[~constant_filter.get_support()]\n",
    "if len(zero_var_cols) > 0:\n",
    "    print(\"\\nâš ï¸ Colonne con varianza nulla:\", zero_var_cols.tolist())\n",
    "else:\n",
    "    print(\"\\nâœ… Nessuna colonna con varianza nulla\")\n",
    "\n",
    "# 5. Statistiche descrittive (prime 10 righe, resto su CSV)\n",
    "desc = df.describe().T\n",
    "print(\"\\nðŸ“ˆ Statistiche di base (prime 10 variabili):\")\n",
    "display(desc.head(10))\n",
    "desc.to_csv(\"../tables/descriptive_postclean.csv\")\n",
    "\n",
    "# 6. Outlier check (boxplot ordinato per varianza, escluso LEDD)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cols_to_plot = [\n",
    "    col for col in df.columns\n",
    "    if df[col].nunique() > 10 and df[col].dtype in ['float64', 'int64'] and col != 'LEDD'\n",
    "]\n",
    "\n",
    "df_melted = pd.melt(df[cols_to_plot])\n",
    "order = df[cols_to_plot].var().sort_values(ascending=False).index\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(x='variable', y='value', data=df_melted, order=order)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"ðŸ“¦ Boxplot delle feature continue (escluso LEDD)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "\n",
    "# STEP 0 â€” Impostazioni\n",
    "log_vars = [c for c in [\"LEDD\", \"Duration (years)\"] if c in df.columns]  # trasformazione log\n",
    "ordinals_to_exclude = [c for c in [\"OnsetCategory\"] if c in df.columns]   # evita ordinali nei test outlier\n",
    "\n",
    "# STEP 1 â€” Seleziona feature numeriche continue (no target/binari/ordinali)\n",
    "cols_to_plot = [\n",
    "    col for col in df.columns\n",
    "    if df[col].dtype in ['float64', 'int64']\n",
    "    and df[col].nunique() > 10\n",
    "    and col not in ordinals_to_exclude\n",
    "]\n",
    "\n",
    "# STEP 2 â€” Funzione outlier via IQR\n",
    "def detect_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return ((series < lower) | (series > upper)).sum()\n",
    "\n",
    "# STEP 3 â€” Report outlier e skewness\n",
    "outlier_report = {}\n",
    "skew_report = {}\n",
    "for col in cols_to_plot:\n",
    "    outlier_report[col] = detect_outliers_iqr(df[col])\n",
    "    skew_report[col] = skew(df[col], nan_policy='omit')\n",
    "\n",
    "df_outlier_skew = pd.DataFrame({\n",
    "    \"n_outliers\": pd.Series(outlier_report),\n",
    "    \"skewness\": pd.Series(skew_report)\n",
    "}).sort_values(by=\"n_outliers\", ascending=False)\n",
    "\n",
    "print(\"ðŸ“Š Report outlier e skewness:\")\n",
    "display(df_outlier_skew)\n",
    "\n",
    "# STEP 4 â€” Regole:\n",
    "#   - log1p su LEDD e Duration (years)\n",
    "#   - winsorize se (outlier > 3) o (|skew| > 1.5), escludendo le variabili loggate\n",
    "def winsorize_series(series, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    lower = series.quantile(lower_quantile)\n",
    "    upper = series.quantile(upper_quantile)\n",
    "    return series.clip(lower, upper)\n",
    "\n",
    "# 4a) Log-transform in-place (manteniamo lo stesso nome per semplicitÃ )\n",
    "for col in log_vars:\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "# 4b) Selezione per winsorizing\n",
    "candidates = df_outlier_skew[\n",
    "    (df_outlier_skew[\"n_outliers\"] > 3) | (df_outlier_skew[\"skewness\"].abs() > 1.5)\n",
    "].index.tolist()\n",
    "\n",
    "features_to_winsorize = [c for c in candidates if c not in log_vars]\n",
    "\n",
    "print(f\"\\nâœ… Feature selezionate per Winsorizing ({len(features_to_winsorize)}):\")\n",
    "print(features_to_winsorize)\n",
    "\n",
    "for col in features_to_winsorize:\n",
    "    df[col] = winsorize_series(df[col])\n",
    "\n",
    "# STEP 5 â€” Verifica visiva (sceglie la prima variabile modificata, se c'Ã¨; altrimenti una qualsiasi continua)\n",
    "if len(features_to_winsorize) > 0:\n",
    "    esempio = features_to_winsorize[0]\n",
    "elif len(cols_to_plot) > 0:\n",
    "    esempio = cols_to_plot[0]\n",
    "else:\n",
    "    esempio = None\n",
    "\n",
    "if esempio is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.boxplot(y=df[esempio], ax=axes[0])\n",
    "    axes[0].set_title(f\"ðŸŸ¥ Dopo Winsorizing/Log â€” {esempio}\")\n",
    "    sns.histplot(df[esempio], ax=axes[1], kde=True)\n",
    "    axes[1].set_title(f\"ðŸ“ˆ Distribuzione dopo trasformazioni â€” {esempio}\")\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Controllo QualitÃ  Preprocessing Outlier\", y=1.02, fontsize=14)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"â„¹ï¸ Nessuna variabile disponibile per il plot di controllo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ”¹ Escludiamo target e variabili non adatte alla correlazione\n",
    "exclude_cols = [\n",
    "    \"target_bin\", \"Hyposmia\", \"REM\", \"Depression\", \"Constipation\",\n",
    "    \"ID\", \"Evaluation_Date\", \"AgeOnset\", \"OnsetCategory\", \n",
    "    \"LEDD\", \"Updrs_III\"\n",
    "]\n",
    "\n",
    "X_numeriche = df.drop(columns=[col for col in exclude_cols if col in df.columns])\n",
    "\n",
    "# ðŸ”¹ Calcolo della matrice di correlazione Spearman\n",
    "corr_matrix = X_numeriche.corr(method=\"spearman\")\n",
    "\n",
    "# ðŸ”¹ Lista per salvare le coppie ad alta correlazione\n",
    "correlated_pairs = []\n",
    "\n",
    "threshold = 0.6  # piÃ¹ stringente\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix.columns)):\n",
    "        corr_value = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > threshold:\n",
    "            f1 = corr_matrix.columns[i]\n",
    "            f2 = corr_matrix.columns[j]\n",
    "            correlated_pairs.append((f1, f2, corr_value))\n",
    "\n",
    "# ðŸ”¹ Ordinamento per forza di correlazione\n",
    "correlated_df = pd.DataFrame(correlated_pairs, columns=[\"Feature_1\", \"Feature_2\", \"Spearman_r\"])\n",
    "correlated_df[\"|r|\"] = correlated_df[\"Spearman_r\"].abs()\n",
    "correlated_df = correlated_df.sort_values(by=\"|r|\", ascending=False).drop(columns=\"|r|\")\n",
    "\n",
    "# ðŸ”¹ Output\n",
    "print(f\"ðŸ”Ž Coppie di feature con Spearman > {threshold}:\")\n",
    "display(correlated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib_venn import venn3\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# --- Palette e label ---\n",
    "palette = {0: \"#003f5c\", 1: \"#bc5090\"}\n",
    "main_color = palette[1]\n",
    "alt_color  = palette[0]\n",
    "\n",
    "# ======================\n",
    "# STEP 0 â€” Rimozione feature non utilizzabili / ridondanti (logica aggiornata)\n",
    "# ======================\n",
    "\n",
    "# a) Colonne da escludere (identificativi, target, prodromi, cliniche pure)\n",
    "base_exclude = [\n",
    "    \"target_bin\", \"ID\", \"Surname\", \"Name\", \"Center\", \"Evaluation_Date\", \"ProdromalCount\",\n",
    "    \"Constipation\", \"Hyposmia\", \"REM\", \"Depression\",\n",
    "    \"Updrs-III\", \"H-Y\"  # scale cliniche (le useremo come covariate nei modelli, non nella FS biomeccanica)\n",
    "]\n",
    "\n",
    "# b) Ridondanze note (da analisi di collinearitÃ ): tieni Gait Speed, droppa Stride Length; droppa Stance & Swing; tra HRV/HRAP tieni HR V\n",
    "redundant_cols = [\n",
    "    \"Stance\", \"Swing\", \"Gait Speed\", \"HR AP\"\n",
    "]\n",
    "\n",
    "# c) Eventuali flag di missingness\n",
    "flag_cols = [c for c in df.columns if c.endswith(\"_was_missing\") or c == \"missing_flags_sum\"]\n",
    "\n",
    "cols_to_drop = [c for c in (base_exclude + redundant_cols + flag_cols) if c in df.columns]\n",
    "\n",
    "df_filtered = df.drop(columns=cols_to_drop, errors=\"ignore\").copy()\n",
    "\n",
    "# X e y\n",
    "y = df[\"target_bin\"].astype(int)\n",
    "X = df_filtered.copy()\n",
    "\n",
    "print(f\"ðŸ”§ FS: partiamo da {X.shape[1]} feature (dopo esclusioni).\")\n",
    "\n",
    "# ======================\n",
    "# STEP 1 â€” FILTER: ANOVA F-test\n",
    "# ======================\n",
    "\n",
    "filter_selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "filter_selector.fit(X, y)\n",
    "\n",
    "filter_scores = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"F_score\": filter_selector.scores_,\n",
    "    \"p_value\": filter_selector.pvalues_\n",
    "}).sort_values(by=\"F_score\", ascending=False)\n",
    "\n",
    "features_filter = filter_scores.query(\"p_value < 0.05\")[\"Feature\"].tolist()\n",
    "\n",
    "# GRAFICO 1 â€” Barplot F-score (solo significanti)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sig_df = filter_scores.query(\"p_value < 0.05\").copy()\n",
    "sns.barplot(data=sig_df, x=\"F_score\", y=\"Feature\", color=main_color)\n",
    "plt.title(\"FILTER â€” ANOVA F-test (p < 0.05)\", fontsize=14)\n",
    "plt.xlabel(\"F-score\"); plt.ylabel(\"\")\n",
    "sns.despine(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ======================\n",
    "# STEP 2 â€” WRAPPER: RFE con RandomForest\n",
    "# ======================\n",
    "\n",
    "# Selezioniamo solo le feature passate dal filtro (se vuote, fallback a tutte)\n",
    "X_filter = X[features_filter] if len(features_filter) > 0 else X.copy()\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rfe_selector = RFE(estimator=rf_clf, n_features_to_select=min(10, X_filter.shape[1]), step=1)\n",
    "rfe_selector.fit(X_filter, y)\n",
    "\n",
    "features_wrapper = X_filter.columns[rfe_selector.support_].tolist()\n",
    "\n",
    "# GRAFICO 2 â€” Ranking RFE (1 = selezionata)\n",
    "rfe_ranking = pd.Series(rfe_selector.ranking_, index=X_filter.columns).sort_values(ascending=True)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=rfe_ranking.values, y=rfe_ranking.index, palette=[alt_color if r>1 else main_color for r in rfe_ranking.values])\n",
    "plt.title(\"WRAPPER â€” RFE Ranking (1 = Selezionata)\", fontsize=14)\n",
    "plt.xlabel(\"Ranking\"); plt.ylabel(\"Feature\")\n",
    "sns.despine(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ======================\n",
    "# STEP 3 â€” EMBEDDED: Feature Importance da RF\n",
    "# ======================\n",
    "\n",
    "rf_clf.fit(X_filter[features_wrapper], y)\n",
    "importances = rf_clf.feature_importances_\n",
    "\n",
    "df_importance = pd.DataFrame({\n",
    "    \"Feature\": features_wrapper,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "mean_importance = df_importance[\"Importance\"].mean()\n",
    "features_embedded = df_importance.loc[df_importance[\"Importance\"] > mean_importance, \"Feature\"].tolist()\n",
    "\n",
    "# GRAFICO 3 â€” Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_importance, x=\"Importance\", y=\"Feature\", color=alt_color)\n",
    "plt.title(\"EMBEDDED â€” Random Forest Feature Importance\", fontsize=14)\n",
    "plt.xlabel(\"Importance Score\"); plt.ylabel(\"\")\n",
    "sns.despine(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ======================\n",
    "# STEP 4 â€” VENN Diagram\n",
    "# ======================\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn3(\n",
    "    [set(features_filter), set(features_wrapper), set(features_embedded)],\n",
    "    set_labels=(\"Filter\", \"Wrapper\", \"Embedded\"),\n",
    "    set_colors=(main_color, alt_color, \"#ffa600\"),\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"Convergenza tra Metodi di Feature Selection\", fontsize=14)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ======================\n",
    "# STEP 5 â€” Output finale\n",
    "# ======================\n",
    "\n",
    "df_selected = df[features_embedded + [\"target_bin\"]].copy()\n",
    "\n",
    "print(\"âœ… FILTER â†’\", len(features_filter), \"feature selezionate\")\n",
    "print(\"âœ… WRAPPER â†’\", len(features_wrapper), \"feature selezionate\")\n",
    "print(\"âœ… EMBEDDED â†’\", len(features_embedded), \"feature finali top\")\n",
    "print(\"\\nðŸŽ¯ FEATURE SELEZIONATE (da passare a CTGAN / modelli):\")\n",
    "print(features_embedded)\n",
    "\n",
    "# ======================\n",
    "# STEP 6 â€” Tabella completa con valori numerici\n",
    "# ======================\n",
    "\n",
    "# 1) F-Score + p-value (Filter)\n",
    "filter_data = filter_scores.set_index(\"Feature\")[[\"F_score\", \"p_value\"]]\n",
    "\n",
    "# 2) RFE ranking (Wrapper)\n",
    "rfe_data = pd.DataFrame({\"RFE_ranking\": rfe_selector.ranking_}, index=X_filter.columns)\n",
    "\n",
    "# 3) Feature Importance (Embedded)\n",
    "embedded_data = df_importance.set_index(\"Feature\")[[\"Importance\"]]\n",
    "\n",
    "# Unione e indicatori di selezione\n",
    "summary_df_complete = pd.concat([filter_data, rfe_data, embedded_data], axis=1)\n",
    "summary_df_complete[\"Filter_selected\"]   = summary_df_complete.index.isin(features_filter).astype(int)\n",
    "summary_df_complete[\"Wrapper_selected\"]  = summary_df_complete.index.isin(features_wrapper).astype(int)\n",
    "summary_df_complete[\"Embedded_selected\"] = summary_df_complete.index.isin(features_embedded).astype(int)\n",
    "summary_df_complete[\"Totale_metodi\"]     = summary_df_complete[[\"Filter_selected\",\"Wrapper_selected\",\"Embedded_selected\"]].sum(axis=1)\n",
    "\n",
    "summary_df_complete = summary_df_complete.sort_values(by=[\"Totale_metodi\",\"Importance\",\"F_score\"], ascending=[False, False, False])\n",
    "\n",
    "print(\"\\nðŸ“Š Tabella dettagliata (Filter/Wrapper/Embedded):\")\n",
    "print(summary_df_complete.fillna(\"\").to_string())\n",
    "\n",
    "# (Opzionale) salvataggio\n",
    "# summary_df_complete.to_excel(\"../output/feature_selection/riepilogo_feature_selection_completo.xlsx\")\n",
    "# df_selected.to_csv(\"../output/feature_selection/selected_features_embedded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ðŸ”¹ CORE FEATURES (selezionate da Filter + Wrapper + Embedded)\n",
    "core_features = [\n",
    "    \"MSE ML\", \n",
    "    \"iHR V\", \n",
    "    \"MSE V\", \n",
    "    \"MSE AP\", \n",
    "    \"Weigth\"\n",
    "]\n",
    "\n",
    "# ðŸ”¹ COVARIATE CLINICHE (per aggiustamento nei modelli)\n",
    "covariates = [\n",
    "    \"Age\",\n",
    "    \"Sex (M=1, F=2)\",\n",
    "    \"H-Y\",\n",
    "    \"Gait Speed\",\n",
    "    \"Duration (years)\"\n",
    "]\n",
    "\n",
    "# ðŸ”¹ Variabili target / prodromi\n",
    "targets = [\n",
    "    \"target_bin\",      # triade vs non-triade\n",
    "    \"Constipation\", \n",
    "    \"Hyposmia\", \n",
    "    \"REM\", \n",
    "    \"Depression\"\n",
    "]\n",
    "\n",
    "# === 1. Dataset core (solo feature principali + target) ===\n",
    "df_core = df[core_features + [\"target_bin\"]].copy()\n",
    "\n",
    "# === 2. Dataset core + covariates (per modelli supervisionati) ===\n",
    "df_core_cov = df[core_features + covariates + [\"target_bin\"]].copy()\n",
    "\n",
    "# === 3. Dataset completo con prodromi (per analisi descrittive) ===\n",
    "df_with_prodromi = df[core_features + covariates + targets].copy()\n",
    "\n",
    "# ðŸ”¹ Percorsi di output\n",
    "csv_core = \"../data/processed/dataset_core.csv\"\n",
    "xlsx_core = \"../data/processed/dataset_core.xlsx\"\n",
    "\n",
    "csv_cov = \"../data/processed/dataset_core_covariates.csv\"\n",
    "xlsx_cov = \"../data/processed/dataset_core_covariates.xlsx\"\n",
    "\n",
    "csv_prod = \"../data/processed/dataset_with_prodromi.csv\"\n",
    "xlsx_prod = \"../data/processed/dataset_with_prodromi.xlsx\"\n",
    "\n",
    "# ðŸ”¹ Salvataggi\n",
    "df_core.to_csv(csv_core, index=False)\n",
    "df_core.to_excel(xlsx_core, index=False)\n",
    "\n",
    "df_core_cov.to_csv(csv_cov, index=False)\n",
    "df_core_cov.to_excel(xlsx_cov, index=False)\n",
    "\n",
    "df_with_prodromi.to_csv(csv_prod, index=False)\n",
    "df_with_prodromi.to_excel(xlsx_prod, index=False)\n",
    "\n",
    "print(\"âœ… Dataset salvati in:\")\n",
    "print(f\"- Core: {csv_core} / {xlsx_core}\")\n",
    "print(f\"- Core + covariate: {csv_cov} / {xlsx_cov}\")\n",
    "print(f\"- Core + covariate + prodromi: {csv_prod} / {xlsx_prod}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
