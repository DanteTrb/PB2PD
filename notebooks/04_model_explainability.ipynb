{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# STEP 0 ‚Äî Setup & Load\n",
    "# =============================\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "FIG_DIR = \"../figurez/global\"\n",
    "OUT_DIR = \"../tables\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- carica split definitivi\n",
    "train = pd.read_csv(\"../data/processed/train_balanced_ctgan.csv\")\n",
    "test  = pd.read_csv(\"../data/processed/test_original.csv\")\n",
    "\n",
    "y_train = train[\"target_bin\"].astype(int).values\n",
    "y_test  = test[\"target_bin\"].astype(int).values\n",
    "\n",
    "# Ordine fisso delle feature (quelle del tuo dataset core+covariate)\n",
    "feature_order = [\n",
    "    \"MSE ML\",\"iHR V\",\"MSE V\",\"MSE AP\",\"Weigth\",\n",
    "    \"Age\",\"Sex (M=1, F=2)\",\"H-Y\",\"Gait Speed\",\"Duration (years)\"\n",
    "]\n",
    "X_train = train[feature_order].copy()\n",
    "X_test  = test[feature_order].copy()\n",
    "\n",
    "print(f\"Train: {X_train.shape}  |  Test: {X_test.shape}\")\n",
    "\n",
    "# =============================\n",
    "# STEP 1 ‚Äî Fit modello finale (Random Forest balanced)\n",
    "# =============================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=3,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Metriche sul test reale\n",
    "p_test = rf.predict_proba(X_test)[:, 1]\n",
    "metrics = {\n",
    "    \"ROC_AUC_test\": round(roc_auc_score(y_test, p_test), 3),\n",
    "    \"PR_AUC_test\": round(average_precision_score(y_test, p_test), 3),\n",
    "    \"Brier_test\": round(brier_score_loss(y_test, p_test), 3)\n",
    "}\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 2) RF Importance vs SHAP (global) ‚Äî FIX per shape (n,feat,classes)\n",
    "# =============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --- 2.1 RF feature importances\n",
    "rf_imp = pd.Series(rf.feature_importances_, index=feature_order, name=\"RF_importance\")\n",
    "rf_imp = rf_imp.sort_values(ascending=False)\n",
    "\n",
    "# --- 2.2 SHAP values\n",
    "explainer = shap.TreeExplainer(rf, data=X_train, feature_names=feature_order)\n",
    "shap_expl = explainer(X_test)      # shap.Explanation\n",
    "shap_vals = shap_expl.values       # pu√≤ essere (n, feat) oppure (n, feat, classes)\n",
    "\n",
    "# Se binario con 3D, prendi la classe positiva (indice 1)\n",
    "if shap_vals.ndim == 3:\n",
    "    shap_vals_pos = shap_vals[:, :, 1]\n",
    "    base_vals_pos = np.array(shap_expl.base_values)[:, 1]\n",
    "    shap_expl_pos = shap.Explanation(\n",
    "        values=shap_vals_pos,\n",
    "        base_values=base_vals_pos,\n",
    "        data=shap_expl.data,\n",
    "        feature_names=feature_order\n",
    "    )\n",
    "else:\n",
    "    shap_vals_pos = shap_vals\n",
    "    shap_expl_pos = shap_expl  # gi√† 2D\n",
    "\n",
    "# mean |SHAP| per feature (importanza globale)\n",
    "shap_imp = pd.Series(np.abs(shap_vals_pos).mean(axis=0), index=feature_order, name=\"SHAP_mean_abs\")\n",
    "shap_imp = shap_imp.sort_values(ascending=False)\n",
    "\n",
    "# --- 2.3 Confronto side-by-side (barh)\n",
    "cmp = (\n",
    "    pd.concat([rf_imp, shap_imp], axis=1)\n",
    "      .reindex(shap_imp.index)  # ordina per SHAP\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cmp.iloc[::-1].plot(kind=\"barh\", ax=plt.gca(), color=[\"#9b59b6\", \"#e74c3c\"])\n",
    "plt.title(\"Importanza globale ‚Äî Random Forest vs SHAP (classe positiva, test)\")\n",
    "plt.xlabel(\"Importanza (scale diverse)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/explain_global_rf_vs_shap.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 2.4 Salva tabella importanze\n",
    "cmp.to_csv(f\"{FIG_DIR}/explain_global_rf_vs_shap_table.csv\", index=True)\n",
    "print(\"‚úÖ Salvati: figura e tabella importanze globali (RF vs SHAP).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 3) SHAP Summary (beeswarm & bar) ‚Äî robusto per multi-output\n",
    "# =============================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# -- se shap_expl √® gi√† calcolato nello step 2, usiamo quello.\n",
    "#    altrimenti, per sicurezza:\n",
    "# shap_expl = explainer(X_test, check_additivity=False)\n",
    "\n",
    "def _to_positive_class_expl(expl, feature_names):\n",
    "    \"\"\"\n",
    "    Rende l'Explanation 2D (n_samples, n_features) per la classe positiva.\n",
    "    Funziona sia se expl.values √® 2D che 3D (multi-output).\n",
    "    \"\"\"\n",
    "    vals = expl.values\n",
    "    base = expl.base_values\n",
    "    data = expl.data\n",
    "\n",
    "    if vals.ndim == 3:\n",
    "        # (n_samples, n_features, n_classes) -> teniamo classe 1\n",
    "        vals_pos = vals[:, :, 1]\n",
    "        base_pos = base[:, 1] if base.ndim == 2 else base\n",
    "        return shap.Explanation(values=vals_pos,\n",
    "                                base_values=base_pos,\n",
    "                                data=data,\n",
    "                                feature_names=feature_names)\n",
    "    elif vals.ndim == 2:\n",
    "        return expl\n",
    "    else:\n",
    "        raise ValueError(f\"Shape SHAP inattesa: {vals.shape}\")\n",
    "\n",
    "# --- 3.1 Seleziona classe positiva\n",
    "shap_expl_pos = _to_positive_class_expl(shap_expl, feature_order)\n",
    "\n",
    "# --- 3.2 Beeswarm\n",
    "plt.figure(figsize=(9, 6))\n",
    "shap.plots.beeswarm(shap_expl_pos, max_display=10, show=False)\n",
    "plt.title(\"SHAP Beeswarm ‚Äî contributi globali (classe positiva, test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/explain_shap_beeswarm.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 3.3 Bar plot |SHAP| medio\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.plots.bar(shap_expl_pos, max_display=10, show=False)\n",
    "plt.title(\"SHAP bar ‚Äî mean |SHAP| (classe positiva, test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/explain_shap_bar.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 4) SHAP Dependence con interazione (robusto, legacy API)\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# 4.1 Estrai i valori SHAP della classe positiva in forma (n_test, n_features)\n",
    "# (compatibile con Explainer moderno e vecchio)\n",
    "if hasattr(shap_expl, \"values\"):\n",
    "    sval = shap_expl.values\n",
    "else:\n",
    "    sval = shap_expl  # fallback\n",
    "\n",
    "# Se sono 3D (n, n_feat, 2 classi), prendi la colonna della classe 1\n",
    "if sval.ndim == 3 and sval.shape[-1] == 2:\n",
    "    shap_vals_pos = sval[:, :, 1]\n",
    "elif sval.ndim == 2:\n",
    "    shap_vals_pos = sval\n",
    "else:\n",
    "    raise ValueError(f\"Formato SHAP inatteso: shape={sval.shape}\")\n",
    "\n",
    "# 4.2 Top-3 feature globali (gi√† calcolate nello step 2 come 'shap_imp')\n",
    "top3 = shap_imp.index[:3].tolist()\n",
    "print(\"Top-3 feature (per dependence):\", top3)\n",
    "\n",
    "# 4.3 Definisci partner d‚Äôinterazione (colore)\n",
    "inter_partner = {\n",
    "    \"MSE ML\": \"iHR V\",        # biomeccanica: entropia mediolaterale √ó variabilit√† verticale\n",
    "    \"Weigth\": \"Gait Speed\",   # antropometria √ó velocit√†\n",
    "    \"Gait Speed\": \"MSE V\"     # dinamica del passo √ó entropia verticale\n",
    "}\n",
    "\n",
    "# 4.4 Disegna e salva\n",
    "for feat in top3:\n",
    "    partner = inter_partner.get(feat, None)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    # NB: dependence_plot (legacy) accetta il nome colonna + matrice shap + X (DF)\n",
    "    shap.dependence_plot(\n",
    "        ind=feat,\n",
    "        shap_values=shap_vals_pos,\n",
    "        features=X_test,\n",
    "        interaction_index=partner,\n",
    "        show=False  # cos√¨ controlliamo noi il salvataggio\n",
    "    )\n",
    "    title = f\"SHAP dependence ‚Äî {feat}\" + (f\" (color: {partner})\" if partner else \"\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    out_path = f\"{FIG_DIR}/explain_dependence_{feat.replace(' ', '_')}.png\"\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Salvato: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 5) Interazioni robuste ‚Äî Friedman H¬≤ + PDP 2D\n",
    "# =============================\n",
    "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "# --- mapping nome -> indice (evita errori con tuple di nomi)\n",
    "_col_idx = {c: i for i, c in enumerate(X_test.columns)}\n",
    "\n",
    "def _norm_pd1d(pdp_obj):\n",
    "    \"\"\"Normalizza l'output di partial_dependence 1D in (grid_1d, avg_1d).\"\"\"\n",
    "    # sklearn >= 1.2 (Bunch)\n",
    "    if hasattr(pdp_obj, \"grid_values\"):\n",
    "        xs   = np.asarray(pdp_obj.grid_values[0]).reshape(-1)\n",
    "        avg  = np.asarray(pdp_obj.average)\n",
    "        # shape attesa: (1, n_grid)\n",
    "        if avg.ndim == 2 and avg.shape[0] == 1:\n",
    "            avg = avg[0]\n",
    "        avg = avg.reshape(-1)\n",
    "        return xs, avg\n",
    "    # vecchie versioni (dict-like)\n",
    "    if isinstance(pdp_obj, dict):\n",
    "        xs   = np.asarray(pdp_obj[\"values\"][0]).reshape(-1)\n",
    "        avg  = np.asarray(pdp_obj[\"average\"])\n",
    "        if avg.ndim == 2 and avg.shape[0] == 1:\n",
    "            avg = avg[0]\n",
    "        avg = avg.reshape(-1)\n",
    "        return xs, avg\n",
    "    raise RuntimeError(\"Formato non riconosciuto per PDP 1D.\")\n",
    "\n",
    "def _norm_pd2d(pdp_obj):\n",
    "    \"\"\"Normalizza l'output di partial_dependence 2D in (grid1, grid2, surface_2d).\"\"\"\n",
    "    # sklearn >= 1.2 (Bunch)\n",
    "    if hasattr(pdp_obj, \"grid_values\"):\n",
    "        g1   = np.asarray(pdp_obj.grid_values[0]).reshape(-1)\n",
    "        g2   = np.asarray(pdp_obj.grid_values[1]).reshape(-1)\n",
    "        avg  = np.asarray(pdp_obj.average)\n",
    "        # atteso: (1, n_g1, n_g2)\n",
    "        if avg.ndim == 3 and avg.shape[0] == 1:\n",
    "            avg = avg[0]\n",
    "        # assicurati 2D\n",
    "        avg = np.asarray(avg)\n",
    "        if avg.ndim != 2:\n",
    "            avg = avg.reshape(len(g1), len(g2))\n",
    "        return g1, g2, avg\n",
    "    # vecchie versioni (dict-like)\n",
    "    if isinstance(pdp_obj, dict):\n",
    "        g1   = np.asarray(pdp_obj[\"values\"][0][0]).reshape(-1)\n",
    "        g2   = np.asarray(pdp_obj[\"values\"][0][1]).reshape(-1)\n",
    "        avg  = np.asarray(pdp_obj[\"average\"][0])\n",
    "        avg  = np.asarray(avg)\n",
    "        if avg.ndim != 2:\n",
    "            avg = avg.reshape(len(g1), len(g2))\n",
    "        return g1, g2, avg\n",
    "    raise RuntimeError(\"Formato non riconosciuto per PDP 2D.\")\n",
    "\n",
    "def _pdp_1d(model, X, feat_name, grid=25):\n",
    "    idx = _col_idx[feat_name]\n",
    "    pd1 = partial_dependence(model, X, [idx], grid_resolution=grid, kind=\"average\")\n",
    "    return _norm_pd1d(pd1)\n",
    "\n",
    "def _pdp_2d(model, X, f1_name, f2_name, grid=25):\n",
    "    i1, i2 = _col_idx[f1_name], _col_idx[f2_name]\n",
    "    pd2 = partial_dependence(model, X, [(i1, i2)], grid_resolution=grid, kind=\"average\")\n",
    "    return _norm_pd2d(pd2)\n",
    "\n",
    "def friedman_H2(model, X, f1, f2, grid=25):\n",
    "    \"\"\"\n",
    "    H¬≤_ij ‚âà Var( f_ij(xi,xj) - fi(xi) - fj(xj) + f0 ) / Var[f(X)],\n",
    "    con f(X)= P(target=1) del modello (RF).\n",
    "    \"\"\"\n",
    "    fX = model.predict_proba(X)[:, 1]\n",
    "    var_fx = float(np.var(fX))\n",
    "    if var_fx <= 1e-12:\n",
    "        return 0.0\n",
    "\n",
    "    # PDP 1D\n",
    "    x1, fi = _pdp_1d(model, X, f1, grid=grid)  # (n1,), (n1,)\n",
    "    x2, fj = _pdp_1d(model, X, f2, grid=grid)  # (n2,), (n2,)\n",
    "\n",
    "    # PDP 2D\n",
    "    g1, g2, fij = _pdp_2d(model, X, f1, f2, grid=grid)  # (n1,), (n2,), (n1,n2)\n",
    "\n",
    "    # allinea dimensioni\n",
    "    Fi = np.repeat(fi.reshape(-1, 1), len(g2), axis=1)  # (n1,n2)\n",
    "    Fj = np.repeat(fj.reshape(1, -1), len(g1), axis=0)  # (n1,n2)\n",
    "\n",
    "    f0 = float(np.mean(fX))\n",
    "    resid = fij - Fi - Fj + f0\n",
    "    H2 = float(np.var(resid) / var_fx)\n",
    "    return max(H2, 0.0)\n",
    "\n",
    "# --- calcolo H¬≤ per tutte le coppie\n",
    "from itertools import combinations\n",
    "pairs = list(combinations(feature_order, 2))\n",
    "H_tbl = []\n",
    "for a, b in pairs:\n",
    "    try:\n",
    "        h2 = friedman_H2(rf, X_test, a, b, grid=21)\n",
    "    except Exception as e:\n",
    "        # se una coppia dovesse fallire per qualunque motivo, registra 0 con nota\n",
    "        print(f\"‚ö†Ô∏è H¬≤ fallito per ({a}, {b}): {e}. Set a 0.\")\n",
    "        h2 = 0.0\n",
    "    H_tbl.append({\"feat_A\": a, \"feat_B\": b, \"H2\": h2})\n",
    "\n",
    "H_df = pd.DataFrame(H_tbl).sort_values(\"H2\", ascending=False).reset_index(drop=True)\n",
    "H_path = f\"{OUT_DIR}/friedman_H_pairs_test.csv\"\n",
    "H_df.to_csv(H_path, index=False)\n",
    "print(f\"‚úÖ Salvata tabella H¬≤ coppie: {H_path}\")\n",
    "print(H_df.head(10))\n",
    "\n",
    "# --- Heatmap sulle 5 feature pi√π importanti (da SHAP globale)\n",
    "focus = list(shap_imp.index[:5])  # top-5 da shap_imp\n",
    "M = pd.DataFrame(0.0, index=focus, columns=focus)\n",
    "for i, j in combinations(focus, 2):\n",
    "    m = H_df[(H_df.feat_A.eq(i) & H_df.feat_B.eq(j)) | (H_df.feat_A.eq(j) & H_df.feat_B.eq(i))]\n",
    "    if not m.empty:\n",
    "        val = float(m.iloc[0][\"H2\"])\n",
    "        M.loc[i, j] = M.loc[j, i] = val\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(M, annot=True, fmt=\".3f\", cmap=\"mako\", cbar_kws={\"label\": \"Friedman H¬≤ (test)\"})\n",
    "plt.title(\"Interazioni (Friedman H¬≤) ‚Äî focus feature (test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/explain_interactions_friedman_focus.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- PDP 2D per la coppia con H¬≤ massimo\n",
    "top_pair = (H_df.loc[0, \"feat_A\"], H_df.loc[0, \"feat_B\"])\n",
    "g1, g2, surf = _pdp_2d(rf, X_test, top_pair[0], top_pair[1], grid=31)\n",
    "\n",
    "plt.figure(figsize=(7.2, 5.6))\n",
    "sns.heatmap(\n",
    "    surf,\n",
    "    xticklabels=np.round(g2, 2),\n",
    "    yticklabels=np.round(g1, 2),\n",
    "    cmap=\"rocket_r\",\n",
    "    cbar_kws={\"label\": \"Partial dependence (P[Triade])\"}\n",
    ")\n",
    "plt.xlabel(top_pair[1]); plt.ylabel(top_pair[0])\n",
    "plt.title(f\"PDP 2D ‚Äî {top_pair[0]} √ó {top_pair[1]} (test)  |  H¬≤={H_df.loc[0,'H2']:.3f}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/explain_pdp2d_top_interaction.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Interactions toolkit ‚Äî FAST (compatibile RF classifier)\n",
    "#  ‚Ä¢ PDP 1D/2D (method='brute'), Friedman H¬≤, bootstrap CIs\n",
    "#  ‚Ä¢ PDP 2D + ALE-like 2D + Interaction Network\n",
    "# =========================================\n",
    "import numpy as np, pandas as pd, os, itertools\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "# --- helper: col-name -> index ---\n",
    "def _feat_idx(X, name_or_idx):\n",
    "    if isinstance(name_or_idx, (int, np.integer)):\n",
    "        return int(name_or_idx)\n",
    "    return int(X.columns.get_loc(name_or_idx))\n",
    "\n",
    "# --- PDP 1D / 2D (RF classifier -> method='brute') ---\n",
    "def _pdp_1d(model, X, feat, grid=11):\n",
    "    i = _feat_idx(X, feat)\n",
    "    pd1 = partial_dependence(model, X, [i], grid_resolution=grid, kind=\"average\", method=\"brute\")\n",
    "    if isinstance(pd1, dict):          # sklearn >= 1.4\n",
    "        fx = pd1[\"average\"][0].ravel()\n",
    "        g1 = pd1[\"values\"][0]\n",
    "    else:                               # sklearn <= 1.3 (Bunch)\n",
    "        fx = pd1.average[0].ravel()\n",
    "        g1 = pd1.grid_values[0]\n",
    "    return np.asarray(g1), np.asarray(fx)\n",
    "\n",
    "def _pdp_2d(model, X, f1, f2, grid=11):\n",
    "    i, j = _feat_idx(X, f1), _feat_idx(X, f2)\n",
    "    pd2 = partial_dependence(model, X, [(i, j)], grid_resolution=grid, kind=\"average\", method=\"brute\")\n",
    "    if isinstance(pd2, dict):          # sklearn >= 1.4\n",
    "        surf = pd2[\"average\"][0]       # (len(g1), len(g2))\n",
    "        g1, g2 = pd2[\"values\"]\n",
    "    else:                               # sklearn <= 1.3\n",
    "        surf = pd2.average[0]\n",
    "        g1, g2 = pd2.grid_values\n",
    "    return np.asarray(g1), np.asarray(g2), np.asarray(surf)\n",
    "\n",
    "# --- Friedman H¬≤ ---\n",
    "def friedman_H2(model, X, f1, f2, grid=11):\n",
    "    g1, f_i  = _pdp_1d(model, X, f1, grid=grid)\n",
    "    g2, f_j  = _pdp_1d(model, X, f2, grid=grid)\n",
    "    G1, G2, f_ij = _pdp_2d(model, X, f1, f2, grid=grid)\n",
    "\n",
    "    Fi = np.repeat(f_i.reshape(-1, 1), len(G2), axis=1)\n",
    "    Fj = np.repeat(f_j.reshape(1, -1), len(G1), axis=0)\n",
    "\n",
    "    fX = model.predict_proba(X)[:, 1]\n",
    "    f0 = float(np.mean(fX))\n",
    "\n",
    "    I = f_ij - Fi - Fj + f0\n",
    "    num = np.var(I, ddof=1)\n",
    "    den = np.var(f_ij, ddof=1)\n",
    "    return float(0.0 if den <= 0 else num / den)\n",
    "\n",
    "# --- Bootstrap CIs (FAST) ---\n",
    "def bootstrap_H2(model, X, pairs, B=80, grid=11, seed=42, save_csv=True, out_dir=\"../tables\"):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows, n = [], len(X)\n",
    "    for a, b in pairs:\n",
    "        vals = []\n",
    "        print(f\"[BOOT] {a} √ó {b} ‚Ä¶\", end=\" \")\n",
    "        for _ in range(B):\n",
    "            idx = rng.integers(0, n, size=n)   # bootstrap sulle righe\n",
    "            Xb = X.iloc[idx]\n",
    "            vals.append(friedman_H2(model, Xb, a, b, grid=grid))\n",
    "        vals = np.array(vals)\n",
    "        rows.append({\n",
    "            \"feat_A\": a, \"feat_B\": b,\n",
    "            \"H2_mean\": vals.mean(),\n",
    "            \"H2_sd\": vals.std(ddof=1),\n",
    "            \"H2_p2.5\": np.quantile(vals, 0.025),\n",
    "            \"H2_p97.5\": np.quantile(vals, 0.975)\n",
    "        })\n",
    "        print(\"done.\")\n",
    "    df = pd.DataFrame(rows).sort_values(\"H2_mean\", ascending=False).reset_index(drop=True)\n",
    "    if save_csv:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        df.to_csv(os.path.join(out_dir, \"friedman_H_pairs_bootstrap_FAST.csv\"), index=False)\n",
    "    return df\n",
    "\n",
    "# --- Plot: barre con CI ---\n",
    "def plot_H2_CI(table, fig_dir=\"../figurez/global\", title=\"Interazioni (Friedman H¬≤) ‚Äî bootstrap 95% CI [FAST]\"):\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    tbl = table.copy()\n",
    "    tbl[\"pair\"] = tbl[\"feat_A\"] + \" √ó \" + tbl[\"feat_B\"]\n",
    "    tbl = tbl.sort_values(\"H2_mean\", ascending=True)\n",
    "\n",
    "    plt.figure(figsize=(8, 5.5))\n",
    "    y = np.arange(len(tbl))\n",
    "    err = np.vstack([tbl[\"H2_mean\"] - tbl[\"H2_p2.5\"], tbl[\"H2_p97.5\"] - tbl[\"H2_mean\"]])\n",
    "    plt.barh(y, tbl[\"H2_mean\"], xerr=err, capsize=4, color=\"#7b4397\", alpha=0.85)\n",
    "    plt.yticks(y, tbl[\"pair\"])\n",
    "    plt.xlabel(\"Friedman H¬≤ (mean, 95% CI)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"friedman_H2_bootstrap_bar_FAST.png\"), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# --- PDP 2D heatmap ---\n",
    "def plot_pdp2d(model, X, pairs, grid=11, fig_dir=\"../figurez/global\"):\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    for a, b in pairs:\n",
    "        g1, g2, surf = _pdp_2d(model, X, a, b, grid=grid)\n",
    "        plt.figure(figsize=(6.5, 5.3))\n",
    "        sns.heatmap(surf, xticklabels=np.round(g2, 2), yticklabels=np.round(g1, 2),\n",
    "                    cmap=\"magma\", cbar_kws={\"label\": \"Partial dependence (P[Triade])\"})\n",
    "        plt.xlabel(b); plt.ylabel(a)\n",
    "        plt.title(f\"PDP 2D ‚Äî {a} √ó {b} (FAST)\")\n",
    "        plt.tight_layout()\n",
    "        safe = f\"{a.replace(' ','_')}_x_{b.replace(' ','_')}\".replace(\"/\",\"-\")\n",
    "        plt.savefig(os.path.join(fig_dir, f\"pdp2d_{safe}_FAST.png\"), dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "# --- ALE 2D ‚Äúlight‚Äù (centrato) ---\n",
    "def ale2d_light(model, X, f1, f2, grid=11):\n",
    "    g1, f_i  = _pdp_1d(model, X, f1, grid=grid)\n",
    "    g2, f_j  = _pdp_1d(model, X, f2, grid=grid)\n",
    "    G1, G2, f_ij = _pdp_2d(model, X, f1, f2, grid=grid)\n",
    "    Fi = np.repeat(f_i.reshape(-1, 1), len(G2), axis=1)\n",
    "    Fj = np.repeat(f_j.reshape(1, -1), len(G1), axis=0)\n",
    "    f0 = float(np.mean(model.predict_proba(X)[:, 1]))\n",
    "    return G1, G2, f_ij - Fi - Fj + f0\n",
    "\n",
    "def plot_ale2d_light(model, X, pairs, grid=11, fig_dir=\"../figurez/global\"):\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    for a, b in pairs:\n",
    "        g1, g2, A = ale2d_light(model, X, a, b, grid=grid)\n",
    "        plt.figure(figsize=(6.5, 5.3))\n",
    "        sns.heatmap(A, xticklabels=np.round(g2, 2), yticklabels=np.round(g1, 2),\n",
    "                    cmap=\"coolwarm\", center=0, cbar_kws={\"label\": \"ALE-like (centrato)\"})\n",
    "        plt.xlabel(b); plt.ylabel(a)\n",
    "        plt.title(f\"ALE 2D (light) ‚Äî {a} √ó {b} (FAST)\")\n",
    "        plt.tight_layout()\n",
    "        safe = f\"{a.replace(' ','_')}_x_{b.replace(' ','_')}\".replace(\"/\",\"-\")\n",
    "        plt.savefig(os.path.join(fig_dir, f\"ale2d_light_{safe}_FAST.png\"), dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "# --- Network delle interazioni ---\n",
    "def plot_interaction_network(H_df, top_k=8, title=\"Network delle interazioni (H¬≤)\", fig_dir=\"../figures\"):\n",
    "    import networkx as nx\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "    Hs = H_df.sort_values(\"H2_mean\", ascending=False).head(top_k)\n",
    "    G = nx.Graph()\n",
    "    for _, r in Hs.iterrows():\n",
    "        G.add_edge(r[\"feat_A\"], r[\"feat_B\"], weight=float(r[\"H2_mean\"]))\n",
    "    pos = nx.spring_layout(G, seed=7, k=1.3)\n",
    "    weights = np.array([d[\"weight\"] for _, _, d in G.edges(data=True)])\n",
    "    lw = 2 + 10 * (weights - weights.min()) / (weights.ptp() + 1e-9)\n",
    "\n",
    "    plt.figure(figsize=(7.8, 6.2))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1600, node_color=\"#f6f1ff\", edgecolors=\"#7b4397\", linewidths=1.5)\n",
    "    nx.draw_networkx_edges(G, pos, width=lw, edge_color=\"#7b4397\", alpha=0.85)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "    edge_labels = {(u, v): f\"{d['weight']:.3f}\" for u, v, d in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"interaction_network_H2_FAST.png\"), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# =========================================\n",
    "# ESECUZIONE (usa rf e X_test gi√† in memoria)\n",
    "# =========================================\n",
    "pairs = [\n",
    "    (\"MSE ML\",\"MSE V\"),\n",
    "    (\"MSE ML\",\"MSE AP\"),\n",
    "    (\"MSE ML\",\"iHR V\"),\n",
    "    (\"Sex (M=1, F=2)\",\"Gait Speed\"),\n",
    "]\n",
    "\n",
    "H_boot = bootstrap_H2(rf, X_test, pairs, B=80, grid=11, seed=42, save_csv=True, out_dir=OUT_DIR)\n",
    "display(H_boot)\n",
    "plot_H2_CI(H_boot, fig_dir=FIG_DIR, title=\"Interazioni (Friedman H¬≤) ‚Äî bootstrap 95% CI [FAST]\")\n",
    "\n",
    "plot_pdp2d(rf, X_test, pairs, grid=11, fig_dir=FIG_DIR)\n",
    "plot_ale2d_light(rf, X_test, pairs, grid=11, fig_dir=FIG_DIR)\n",
    "\n",
    "plot_interaction_network(H_boot, top_k=len(pairs), title=\"Network delle interazioni ‚Äî H¬≤ (bootstrap mean, FAST)\", fig_dir=FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper: estrai Explanation 1D (classe positiva) per un singolo caso ===\n",
    "def get_ex_row_for_class(shap_expl_full, i, class_index=1):\n",
    "    \"\"\"\n",
    "    Ritorna una shap.Explanation 1D per la riga i e la classe `class_index`.\n",
    "    Gestisce i casi in cui .values √® (n_feat,) oppure (n_feat, n_class).\n",
    "    \"\"\"\n",
    "    ex_all = shap_expl_full[i]            # Explanation \"row\"\n",
    "    vals = ex_all.values                  # (n_feat,) oppure (n_feat, n_class)\n",
    "\n",
    "    if vals.ndim == 1:\n",
    "        # gi√† 1D: restituisco cos√¨ com'√®\n",
    "        return ex_all\n",
    "\n",
    "    # vals √® (n_feat, n_class) -> seleziona la colonna della classe\n",
    "    k = class_index if vals.shape[1] > class_index else vals.shape[1] - 1\n",
    "\n",
    "    # base_values pu√≤ essere scalar o (n_class,)\n",
    "    base = ex_all.base_values\n",
    "    if np.ndim(base) == 0:\n",
    "        base_k = float(base)\n",
    "    else:\n",
    "        base_k = float(base[k])\n",
    "\n",
    "    # costruiamo una Explanation 1D per waterfall\n",
    "    ex_1d = shap.Explanation(\n",
    "        values     = vals[:, k],\n",
    "        base_values= base_k,\n",
    "        data       = ex_all.data,\n",
    "        feature_names = ex_all.feature_names\n",
    "    )\n",
    "    return ex_1d\n",
    "\n",
    "# === 6.x rigenero i plot locali e le tabelle usando l'Explanation 1D ===\n",
    "p_test = rf.predict_proba(X_test)[:, 1]\n",
    "idx_examples = pick_examples(y_test, p_test, thr=0.40)\n",
    "\n",
    "records = []\n",
    "for i in idx_examples:\n",
    "    # Explanation 1D per la classe positiva\n",
    "    ex = get_ex_row_for_class(shap_expl, i, class_index=1)\n",
    "    x_row = X_test.iloc[i]\n",
    "\n",
    "    # --- Waterfall (ora 1D)\n",
    "    plt.figure(figsize=(7.2, 5.2))\n",
    "    shap.plots.waterfall(ex, max_display=12, show=False)\n",
    "    plt.title(f\"Waterfall ‚Äî id={i} | y={y_test[i]} | p={p_test[i]:.2f}\")\n",
    "    fn_wf = f\"{FIG_DIR}/local_waterfall_id{i}.png\"\n",
    "    plt.tight_layout(); plt.savefig(fn_wf, dpi=300); plt.show()\n",
    "\n",
    "    # --- Top-k contributi (uso ex.values 1D)\n",
    "    vals_1d = np.array(ex.values)\n",
    "    order = np.argsort(-np.abs(vals_1d))[:8]\n",
    "    for j in order:\n",
    "        fname = feature_order[j]\n",
    "        val = float(vals_1d[j])\n",
    "        records.append({\n",
    "            \"id\": int(i),\n",
    "            \"y_true\": int(y_test[i]),\n",
    "            \"p_pred\": float(p_test[i]),\n",
    "            \"feature\": fname,\n",
    "            \"x_value\": float(x_row[fname]),\n",
    "            \"shap_value\": val,\n",
    "            \"direction\": \"‚Üë rischio\" if val > 0 else \"‚Üì rischio\"\n",
    "        })\n",
    "\n",
    "# salva la tabella top-contribuzioni\n",
    "top_tbl = pd.DataFrame(records)\n",
    "top_tbl.to_csv(f\"{OUT_DIR}/local_top_contributions.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Waterfall salvati e tabella top-contribuzioni aggiornata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# STEP 7 ‚Äî Slice explainability: |SHAP| per classe\n",
    "# =============================\n",
    "import numpy as np, pandas as pd\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# --- 7.0 Prepara matrice |SHAP| (classe positiva)\n",
    "# Se shap_expl √® multi-output, seleziona la colonna della classe positiva (1)\n",
    "if hasattr(shap_expl, \"values\"):\n",
    "    vals = shap_expl.values\n",
    "else:\n",
    "    vals = np.array(shap_expl)  # fallback\n",
    "\n",
    "if vals.ndim == 3:   # (n_samples, n_features, 2) o (n, 2, n_feat)\n",
    "    # prova a individuare l'asse delle classi\n",
    "    if vals.shape[-1] == 2:\n",
    "        vals = vals[..., 1]                # prendi classe 1 sull'ultimo asse\n",
    "    elif vals.shape[1] == 2:\n",
    "        vals = vals[:, 1, :]               # prendi classe 1 sul secondo asse\n",
    "    else:\n",
    "        raise ValueError(\"Forma SHAP inattesa; specifica manualmente la classe positiva.\")\n",
    "\n",
    "abs_shap = pd.DataFrame(np.abs(vals), columns=feature_order)\n",
    "abs_shap[\"target_bin\"] = pd.Series(y_test).astype(int)\n",
    "\n",
    "# --- 7.1 Long format per plotting\n",
    "melt = abs_shap.melt(id_vars=\"target_bin\", var_name=\"feature\", value_name=\"abs_shap\")\n",
    "\n",
    "# Ordina le feature per importanza media (classe positiva)\n",
    "order = (\n",
    "    melt.groupby(\"feature\")[\"abs_shap\"]\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .index.tolist()\n",
    ")\n",
    "\n",
    "# --- 7.2 Boxplot + swarm per classe\n",
    "plt.figure(figsize=(11, 6))\n",
    "sns.boxplot(\n",
    "    data=melt, x=\"feature\", y=\"abs_shap\", hue=\"target_bin\",\n",
    "    order=order, palette=[\"#003f5c\", \"#bc5090\"], fliersize=0, linewidth=1.2\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=melt, x=\"feature\", y=\"abs_shap\", hue=\"target_bin\",\n",
    "    order=order, dodge=True, palette=[\"#003f5c55\", \"#bc509055\"], size=3, alpha=0.6\n",
    ")\n",
    "plt.legend(title=\"Classe\", loc=\"upper right\", labels=[\"0 = No Triade\", \"1 = Triade\"])\n",
    "plt.xticks(rotation=40, ha=\"right\")\n",
    "plt.ylabel(\"|SHAP|\")\n",
    "plt.xlabel(\"\")\n",
    "plt.title(\"Distribuzione |SHAP| per classe (test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/slice_abs_shap_box_swarm.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 7.3 (Opzionale) Violin split\n",
    "plt.figure(figsize=(11, 6))\n",
    "sns.violinplot(\n",
    "    data=melt, x=\"feature\", y=\"abs_shap\", hue=\"target_bin\",\n",
    "    order=order, split=True, inner=\"quartile\", palette=[\"#003f5c\", \"#bc5090\"]\n",
    ")\n",
    "plt.legend(title=\"Classe\", loc=\"upper right\", labels=[\"0 = No Triade\", \"1 = Triade\"])\n",
    "plt.xticks(rotation=40, ha=\"right\")\n",
    "plt.ylabel(\"|SHAP|\")\n",
    "plt.xlabel(\"\")\n",
    "plt.title(\"Raincloud/Violin split ‚Äî |SHAP| per classe (test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/slice_abs_shap_violin_split.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 7.4 Statistiche descrittive + test tra classi (Mann‚ÄìWhitney)\n",
    "rows = []\n",
    "for feat in feature_order:\n",
    "    v0 = abs_shap.loc[abs_shap[\"target_bin\"] == 0, feat].values\n",
    "    v1 = abs_shap.loc[abs_shap[\"target_bin\"] == 1, feat].values\n",
    "    # robust summary\n",
    "    q0 = np.percentile(v0, [25, 50, 75])\n",
    "    q1 = np.percentile(v1, [25, 50, 75])\n",
    "    # test non-parametrico\n",
    "    try:\n",
    "        stat, p = mannwhitneyu(v0, v1, alternative=\"two-sided\")\n",
    "    except ValueError:\n",
    "        stat, p = np.nan, np.nan\n",
    "    rows.append({\n",
    "        \"feature\": feat,\n",
    "        \"n_class0\": len(v0), \"mean_class0\": np.mean(v0), \"median_class0\": np.median(v0),\n",
    "        \"q25_class0\": q0[0], \"q75_class0\": q0[2], \"IQR_class0\": q0[2]-q0[0],\n",
    "        \"n_class1\": len(v1), \"mean_class1\": np.mean(v1), \"median_class1\": np.median(v1),\n",
    "        \"q25_class1\": q1[0], \"q75_class1\": q1[2], \"IQR_class1\": q1[2]-q1[0],\n",
    "        \"MWU_stat\": stat, \"MWU_pvalue\": p\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(rows).sort_values(\"mean_class1\", ascending=False)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "stats_df.to_csv(f\"{OUT_DIR}/slice_abs_shap_stats.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Salvati:\")\n",
    "print(f\"- Figura: {FIG_DIR}/slice_abs_shap_box_swarm.png\")\n",
    "print(f\"- Figura: {FIG_DIR}/slice_abs_shap_violin_split.png\")\n",
    "print(f\"- Tabella: {OUT_DIR}/slice_abs_shap_stats.csv\")\n",
    "stats_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 7b ‚Äî Effect size (Cliff's Œ¥) + FDR + interpretazione\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# helper Cliff‚Äôs Œ¥ + categorizzazione\n",
    "def cliffs_delta(x, y):\n",
    "    nx, ny = len(x), len(y)\n",
    "    gt = sum(xi > yj for xi in x for yj in y)\n",
    "    lt = sum(xi < yj for xi in x for yj in y)\n",
    "    delta = (gt - lt) / (nx * ny)\n",
    "    return delta\n",
    "\n",
    "def interpret_delta(delta):\n",
    "    ad = abs(delta)\n",
    "    if ad < 0.147: return \"negligible\"\n",
    "    elif ad < 0.33: return \"small\"\n",
    "    elif ad < 0.474: return \"medium\"\n",
    "    else: return \"large\"\n",
    "\n",
    "# dataframe SHAP assoluti\n",
    "abs_shap = pd.DataFrame(np.abs(shap_expl.values[:,:,1]), columns=feature_order)\n",
    "abs_shap[\"target_bin\"] = y_test\n",
    "\n",
    "rows = []\n",
    "for feat in feature_order:\n",
    "    vals0 = abs_shap.loc[abs_shap[\"target_bin\"]==0, feat].values\n",
    "    vals1 = abs_shap.loc[abs_shap[\"target_bin\"]==1, feat].values\n",
    "    \n",
    "    stat, pval = mannwhitneyu(vals0, vals1, alternative=\"two-sided\")\n",
    "    delta = cliffs_delta(vals1, vals0)   # positivo = maggiore nei Triade\n",
    "    \n",
    "    rows.append({\n",
    "        \"feature\": feat,\n",
    "        \"mean_noTriade\": np.mean(vals0),\n",
    "        \"mean_Triade\": np.mean(vals1),\n",
    "        \"median_noTriade\": np.median(vals0),\n",
    "        \"median_Triade\": np.median(vals1),\n",
    "        \"MWU_p\": pval,\n",
    "        \"Cliffs_delta\": delta,\n",
    "        \"Effect_size\": interpret_delta(delta)\n",
    "    })\n",
    "\n",
    "tbl = pd.DataFrame(rows)\n",
    "\n",
    "# correzione FDR\n",
    "tbl[\"p_adj\"] = multipletests(tbl[\"MWU_p\"], method=\"fdr_bh\")[1]\n",
    "\n",
    "# ordina per Œ¥\n",
    "tbl = tbl.sort_values(\"Cliffs_delta\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# salva tabella\n",
    "tbl.to_csv(f\"{OUT_DIR}/shap_class_contrast.csv\", index=False)\n",
    "display(tbl)\n",
    "\n",
    "# ============================================\n",
    "# Plot: Cliff‚Äôs Œ¥ per feature\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "colors = [\"#e74c3c\" if d>0 else \"#3498db\" for d in tbl[\"Cliffs_delta\"]]\n",
    "plt.barh(tbl[\"feature\"], tbl[\"Cliffs_delta\"], color=colors, alpha=0.8)\n",
    "plt.axvline(0, color=\"k\", lw=1)\n",
    "plt.xlabel(\"Cliff‚Äôs Œ¥ (Triade vs No Triade)\")\n",
    "plt.title(\"Effect size su |SHAP| per classe (test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/shap_class_contrast_cliffs_delta.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# STEP 8 ‚Äî Surrogate interpretable + regole cliniche\n",
    "# =============================\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,\n",
    "                             precision_score, recall_score, confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- directory output (come negli step precedenti)\n",
    "FIG_DIR = \"../figurez/global\"\n",
    "OUT_DIR = \"../tables\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- fallback: se rf/X_* non esistono nel workspace, ricarico e rialleno rapidamente\n",
    "try:\n",
    "    rf, X_train, X_test, y_train, y_test\n",
    "except NameError:\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import brier_score_loss, average_precision_score\n",
    "\n",
    "    feature_order = [\n",
    "        \"MSE ML\",\"iHR V\",\"MSE V\",\"MSE AP\",\"Weigth\",\"Age\",\n",
    "        \"Sex (M=1, F=2)\",\"H-Y\",\"Gait Speed\",\"Duration (years)\"\n",
    "    ]\n",
    "    train = pd.read_csv(\"../data/processed/train_balanced_ctgan.csv\")\n",
    "    test  = pd.read_csv(\"../data/processed/test_original.csv\")\n",
    "\n",
    "    y_train = train[\"target_bin\"].astype(int).values\n",
    "    y_test  = test[\"target_bin\"].astype(int).values\n",
    "    X_train = train[feature_order].copy()\n",
    "    X_test  = test[feature_order].copy()\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=800, min_samples_leaf=3,\n",
    "        class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "# ==============\n",
    "# 1) Etichette RF a soglia clinica\n",
    "# ==============\n",
    "thr = 0.40                                  # soglia clinica scelta negli step precedenti\n",
    "p_train = rf.predict_proba(X_train)[:,1]\n",
    "p_test  = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "y_rf_train = (p_train >= thr).astype(int)   # ‚Äúregola clinica‚Äù del RF sul train\n",
    "y_rf_test  = (p_test  >= thr).astype(int)   # stessa regola sul test (per fidelity)\n",
    "\n",
    "# ==============\n",
    "# 2) Allena surrogate tree per imitare il RF\n",
    "# ==============\n",
    "sur = DecisionTreeClassifier(\n",
    "    max_depth=3,            # profondo quanto basta per regole leggibili\n",
    "    min_samples_leaf=12,    # foglie stabili\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\" # per non inseguire solo la maggioranza\n",
    ")\n",
    "sur.fit(X_train, y_rf_train)\n",
    "\n",
    "# ==============\n",
    "# 3) Fidelity (surrogate vs RF) e performance vs verit√† a terra\n",
    "# ==============\n",
    "# Fidelity (accordo con la decisione RF @ thr)\n",
    "y_rf_hat_test = sur.predict(X_test)\n",
    "fidelity = (y_rf_hat_test == y_rf_test).mean()\n",
    "\n",
    "# Performance surrogate vs ground truth (usando p_sur come proba)\n",
    "p_sur_test = sur.predict_proba(X_test)[:,1]\n",
    "y_sur_test = (p_sur_test >= thr).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, y_sur_test)\n",
    "f1  = f1_score(y_test, y_sur_test)\n",
    "prec = precision_score(y_test, y_sur_test)\n",
    "rec  = recall_score(y_test, y_sur_test)\n",
    "auc  = roc_auc_score(y_test, p_sur_test)\n",
    "\n",
    "print(\"=== Surrogate Decision Tree (max_depth=3) ===\")\n",
    "print(f\"Fidelity a RF@{thr:.2f} (test): {fidelity:.3f}\")\n",
    "print(f\"Acc: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f} | ROC AUC: {auc:.3f}\")\n",
    "\n",
    "# Confusion matrix surrogate vs ground truth\n",
    "cm = confusion_matrix(y_test, y_sur_test)\n",
    "print(\"Confusion Matrix (surrogate vs y_test):\")\n",
    "print(cm)\n",
    "\n",
    "# ==============\n",
    "# 4) Visualizza e salva l‚Äôalbero\n",
    "# ==============\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(\n",
    "    sur,\n",
    "    feature_names=list(X_train.columns),\n",
    "    class_names=[f\"No Triade (thr={thr})\", f\"Triade (thr={thr})\"],\n",
    "    filled=True, rounded=True, impurity=False, proportion=True, fontsize=9\n",
    ")\n",
    "plt.title(f\"Surrogate Tree (depth={sur.get_depth()}, fidelity={fidelity:.2f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/surrogate_tree_depth{sur.get_depth()}_thr{int(thr*100):02d}.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Salva anche il testo delle regole\n",
    "rules_txt = export_text(sur, feature_names=list(X_train.columns), decimals=3)\n",
    "with open(f\"{OUT_DIR}/surrogate_rules_depth{sur.get_depth()}_thr{int(thr*100):02d}.txt\",\"w\") as f:\n",
    "    f.write(rules_txt)\n",
    "print(\"\\n--- Regole (snippet) ---\")\n",
    "print(\"\\n\".join(rules_txt.splitlines()[:20]))\n",
    "\n",
    "# ==============\n",
    "# 5) Estrai regole ‚Äúcliniche‚Äù in tabella (supporto + prevalenze)\n",
    "# ==============\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_leaf_paths(tree_model, feature_names):\n",
    "    \"\"\"Ritorna l‚Äôelenco dei path (regole) fino alle foglie.\"\"\"\n",
    "    tree = tree_model.tree_\n",
    "    paths = []\n",
    "    def recurse(node, current):\n",
    "        if tree.feature[node] != -2:\n",
    "            feat = feature_names[tree.feature[node]]\n",
    "            thr  = tree.threshold[node]\n",
    "            recurse(tree.children_left[node],  current + [f\"{feat} <= {thr:.3f}\"])\n",
    "            recurse(tree.children_right[node], current + [f\"{feat} > {thr:.3f}\"])\n",
    "        else:\n",
    "            paths.append(current)\n",
    "    recurse(0, [])\n",
    "    return paths\n",
    "\n",
    "def leaf_index_for_rows(tree_model, X):\n",
    "    \"\"\"Indice foglia per ogni riga di X.\"\"\"\n",
    "    return tree_model.apply(X)\n",
    "\n",
    "# mappa leaf -> path\n",
    "paths = extract_leaf_paths(sur, list(X_train.columns))\n",
    "leaf_ids = np.unique(sur.apply(X_train))\n",
    "leaf_to_path = {leaf: paths[i] for i, leaf in enumerate(leaf_ids)}\n",
    "\n",
    "# compone tabella su TEST (pi√π onesta)\n",
    "leaf_test = leaf_index_for_rows(sur, X_test)\n",
    "df_leaf = pd.DataFrame({\n",
    "    \"leaf\": leaf_test,\n",
    "    \"y_test\": y_test,\n",
    "    \"y_rf@thr\": y_rf_test,\n",
    "    \"p_sur\": p_sur_test\n",
    "})\n",
    "\n",
    "summary = (\n",
    "    df_leaf.groupby(\"leaf\")\n",
    "          .agg(n=(\"leaf\",\"size\"),\n",
    "               rf_pos=(\"y_rf@thr\",\"mean\"),\n",
    "               true_pos=(\"y_test\",\"mean\"),\n",
    "               p_sur_mean=(\"p_sur\",\"mean\"))\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# aggiungi testo regola\n",
    "summary[\"rule\"] = summary[\"leaf\"].map(leaf_to_path).apply(lambda x: \" AND \".join(x))\n",
    "summary = summary.sort_values(\"true_pos\", ascending=False)\n",
    "\n",
    "# salva CSV e mostra top\n",
    "csv_path = f\"{OUT_DIR}/surrogate_rule_table_test_thr{int(thr*100):02d}.csv\"\n",
    "summary.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nüìÑ Salvata tabella regole: {csv_path}\")\n",
    "print(\"\\nüîù Top-5 regole (per prevalenza reale di Triade nel test):\")\n",
    "display(summary.head(5))\n",
    "\n",
    "# ==============\n",
    "# 6) Mini ‚Äúdecision list‚Äù leggibile (prime 3-4 regole ad alta prevalenza)\n",
    "# ==============\n",
    "top_rules = summary.head(4).copy()\n",
    "def humanize(rule):\n",
    "    # compattazione estetica delle soglie\n",
    "    return rule.replace(\" <= \", \" ‚â§ \").replace(\" > \", \" > \")\n",
    "print(\"\\nüìù Decision list (prime regole):\")\n",
    "for k,(i,row) in enumerate(top_rules.iterrows(),1):\n",
    "    print(f\"{k}) IF {humanize(row['rule'])}  THEN  P_RF‚âà{row['rf_pos']:.2f}  |  P_real‚âà{row['true_pos']:.2f}  (n={int(row['n'])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 8 ‚Äî Surrogate interpretable potenziato\n",
    "# ============================================\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 8.1 Fit surrogate tree su predizioni RF\n",
    "y_hat_train = (rf.predict_proba(X_train)[:,1] >= 0.40).astype(int)\n",
    "sur = DecisionTreeClassifier(max_depth=3, min_samples_leaf=12, random_state=42)\n",
    "sur.fit(X_train, y_hat_train)\n",
    "\n",
    "y_hat_test = (rf.predict_proba(X_test)[:,1] >= 0.40).astype(int)\n",
    "sur_pred = sur.predict(X_test)\n",
    "fidelity = (sur_pred == y_hat_test).mean()\n",
    "print(f\"Surrogate fidelity (test) vs RF@0.40: {fidelity:.3f}\")\n",
    "\n",
    "# --- 8.2 Tabella regole con CI binomiale\n",
    "from collections import deque\n",
    "\n",
    "def extract_rules_with_CI(tree_model, X, y_true):\n",
    "    tree = tree_model.tree_\n",
    "    feature_names = list(X.columns)\n",
    "    rules = []\n",
    "\n",
    "    def recurse(node, path):\n",
    "        if tree.feature[node] != -2:  # non-leaf\n",
    "            feat = feature_names[tree.feature[node]]\n",
    "            thr = tree.threshold[node]\n",
    "            recurse(tree.children_left[node], path + [f\"{feat} <= {thr:.3f}\"])\n",
    "            recurse(tree.children_right[node], path + [f\"{feat} > {thr:.3f}\"])\n",
    "        else:\n",
    "            idx = np.where(tree_model.apply(X) == node)[0]\n",
    "            n = len(idx)\n",
    "            if n == 0: return\n",
    "            true_pos = (y_true[idx] == 1).sum()\n",
    "            p_real = true_pos / n\n",
    "            ci_low, ci_up = proportion_confint(true_pos, n, alpha=0.05, method=\"wilson\")\n",
    "            rules.append({\n",
    "                \"path\": path, \"n\": n,\n",
    "                \"true_pos\": int(true_pos),\n",
    "                \"p_real\": round(p_real, 3),\n",
    "                \"CI95\": f\"[{ci_low:.2f}, {ci_up:.2f}]\"\n",
    "            })\n",
    "    recurse(0, [])\n",
    "    return pd.DataFrame(rules).sort_values(\"p_real\", ascending=False)\n",
    "\n",
    "rules_df = extract_rules_with_CI(sur, X_test, y_test)\n",
    "rules_df.to_csv(f\"{OUT_DIR}/surrogate_rules_CI.csv\", index=False)\n",
    "display(rules_df.head(10))\n",
    "\n",
    "# --- 8.3 Plot albero con soglie arrotondate (clinico)\n",
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(sur, feature_names=X_train.columns, class_names=[\"No Triade\",\"Triade\"],\n",
    "          filled=True, rounded=True, proportion=True, fontsize=9)\n",
    "plt.title(f\"Surrogate Tree (depth=3, fidelity={fidelity:.2f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/surrogate_tree_clinical.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 8.4 Cross-validation stabilit√† surrogate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "feat_counts = []\n",
    "for tr, te in cv.split(X_train, y_train):\n",
    "    X_tr, y_tr = X_train.iloc[tr], (rf.predict_proba(X_train.iloc[tr])[:,1] >= 0.40).astype(int)\n",
    "    sur_cv = DecisionTreeClassifier(max_depth=3, min_samples_leaf=12, random_state=42)\n",
    "    sur_cv.fit(X_tr, y_tr)\n",
    "    # estrai features usate\n",
    "    feats = [X_train.columns[i] for i in sur_cv.tree_.feature if i >= 0]\n",
    "    feat_counts.extend(feats)\n",
    "\n",
    "feat_stability = pd.Series(feat_counts).value_counts(normalize=True).rename(\"freq\")\n",
    "feat_stability.to_csv(f\"{OUT_DIR}/surrogate_feature_stability.csv\")\n",
    "print(\"Feature stability across CV:\\n\", feat_stability)\n",
    "\n",
    "# --- 8.5 Flowchart clinico (logica manuale dalle prime regole)\n",
    "flowchart = \"\"\"\n",
    "Flowchart decisionale (soglie arrotondate):\n",
    "\n",
    "1) Se MSE ML > ~1.75 ‚Üí Triade (alto rischio).\n",
    "   - Se peso <= ~162.5 kg e MSE ML > ~1.81 ‚Üí Triade certo.\n",
    "   - Se peso > ~162.5 kg ‚Üí Triade probabile (~75%).\n",
    "\n",
    "2) Se MSE ML ‚â§ ~1.75:\n",
    "   - Se Gait Speed ‚â§ ~1.40:\n",
    "       - Se MSE V ‚â§ ~1.55 ‚Üí No Triade (‚âà70%).\n",
    "       - Se MSE V > ~1.55 ‚Üí Triade (‚âà50%).\n",
    "   - Se Gait Speed > ~1.40:\n",
    "       - Se MSE V ‚â§ ~1.33 ‚Üí Triade (isolato, n=1).\n",
    "       - Altrimenti ‚Üí Triade (pattern raro).\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{OUT_DIR}/surrogate_flowchart.txt\",\"w\") as f:\n",
    "    f.write(flowchart)\n",
    "\n",
    "print(flowchart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 8.a) Reliability per foglia (surrogate) =========\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.tree import _tree\n",
    "\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) assegna ogni caso test a una foglia del surrogate\n",
    "leaf_id = sur.apply(X_test)  # shape (n_test,)\n",
    "df_leaf = pd.DataFrame({\n",
    "    \"leaf\": leaf_id,\n",
    "    \"y\": y_test.astype(int),\n",
    "    \"p_rf\": p_test\n",
    "})\n",
    "\n",
    "# 2) aggrega per foglia\n",
    "grp = df_leaf.groupby(\"leaf\")\n",
    "tab = grp.agg(n=(\"y\",\"size\"),\n",
    "              event=(\"y\",\"sum\"),\n",
    "              p_real=(\"y\",\"mean\"),\n",
    "              p_rf_mean=(\"p_rf\",\"mean\"),\n",
    "              p_rf_sd=(\"p_rf\",\"std\")).reset_index()\n",
    "\n",
    "# Wilson CI per la prevalenza reale\n",
    "def wilson(p, n, z=1.96):\n",
    "    if n == 0: return (np.nan, np.nan)\n",
    "    denom = 1 + z**2/n\n",
    "    centre = (p + z*z/(2*n))/denom\n",
    "    half = z*np.sqrt((p*(1-p)/n) + (z*z/(4*n*n)))/denom\n",
    "    return (max(0, centre-half), min(1, centre+half))\n",
    "\n",
    "ci_l, ci_u = [], []\n",
    "for _, r in tab.iterrows():\n",
    "    l, u = wilson(r[\"p_real\"], int(r[\"n\"]))\n",
    "    ci_l.append(l); ci_u.append(u)\n",
    "tab[\"p_real_l\"] = ci_l\n",
    "tab[\"p_real_u\"] = ci_u\n",
    "\n",
    "tab = tab.sort_values(\"p_real\", ascending=True).reset_index(drop=True)\n",
    "tab.to_csv(f\"{OUT_DIR}/surrogate_leaf_reliability.csv\", index=False)\n",
    "\n",
    "# 3) plot\n",
    "plt.figure(figsize=(8.5,6))\n",
    "x = np.arange(len(tab))\n",
    "plt.errorbar(x, tab[\"p_real\"], \n",
    "             yerr=[tab[\"p_real\"]-tab[\"p_real_l\"], tab[\"p_real_u\"]-tab[\"p_real\"]],\n",
    "             fmt=\"o\", capsize=4, label=\"Prevalenza reale (95% CI)\", color=\"#2c7fb8\")\n",
    "plt.scatter(x, tab[\"p_rf_mean\"], marker=\"s\", s=60, label=\"Probabilit√† media RF\", color=\"#de2d26\")\n",
    "plt.plot([x.min()-0.5, x.max()+0.5], [0.4, 0.4], ls=\"--\", c=\"gray\", alpha=0.5)  # soglia 0.40\n",
    "plt.xticks(x, [f\"L{int(l)} (n={n})\" for l,n in zip(tab[\"leaf\"], tab[\"n\"])], rotation=30, ha=\"right\")\n",
    "plt.ylim(-0.02, 1.02)\n",
    "plt.ylabel(\"Frequenza / Probabilit√†\")\n",
    "plt.title(\"Mini-reliability per foglia del surrogate (test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/surrogate_leaf_reliability.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìù Salvate:\", \n",
    "      f\"{OUT_DIR}/surrogate_leaf_reliability.csv\", \n",
    "      f\"{FIG_DIR}/surrogate_leaf_reliability.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 8.b) Decision Curve Analysis (RF vs Surrogate) =========\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def net_benefit(y_true, y_pred_binary, pt):\n",
    "    \"\"\"y_pred_binary: decisione (0/1) a soglia pt; pt = threshold probability\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "    n = tp + fp + tn + fn\n",
    "    if n == 0: return np.nan\n",
    "    w = pt / (1 - pt)\n",
    "    return (tp/n) - (fp/n)*w\n",
    "\n",
    "def dca_curve(y_true, proba, pts=np.linspace(0.1, 0.9, 41)):\n",
    "    rows = []\n",
    "    for pt in pts:\n",
    "        pred = (proba >= pt).astype(int)\n",
    "        nb = net_benefit(y_true, pred, pt)\n",
    "        rows.append({\"pt\": pt, \"NB\": nb})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# proba RF gi√† in p_test; surrogate proba: usa predict_proba se disponibile, altrimenti decisione dura\n",
    "if hasattr(sur, \"predict_proba\"):\n",
    "    p_sur = sur.predict_proba(X_test)[:,1]\n",
    "else:\n",
    "    p_sur = sur.predict(X_test)  # 0/1; usata come probabilit√† ‚Äústep-like‚Äù\n",
    "\n",
    "pts = np.linspace(0.1, 0.9, 41)\n",
    "dca_rf  = dca_curve(y_test, p_test, pts);  dca_rf[\"model\"]  = \"RF\"\n",
    "dca_sur = dca_curve(y_test, p_sur,  pts);  dca_sur[\"model\"] = \"Surrogate\"\n",
    "\n",
    "# strategie triviali\n",
    "preval = np.mean(y_test)\n",
    "nb_treat_all = pd.DataFrame({\"pt\": pts, \"NB\": preval - (1-preval)*(pts/(1-pts)), \"model\": \"Tratta tutti\"})\n",
    "nb_treat_none = pd.DataFrame({\"pt\": pts, \"NB\": 0.0, \"model\": \"Tratta nessuno\"})\n",
    "\n",
    "dca = pd.concat([dca_rf, dca_sur, nb_treat_all, nb_treat_none], ignore_index=True)\n",
    "dca.to_csv(f\"{OUT_DIR}/decision_curve_rf_sur.csv\", index=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8.2,6))\n",
    "for name, dfm in dca.groupby(\"model\"):\n",
    "    ls = \"-\" if name in [\"RF\",\"Surrogate\"] else \"--\"\n",
    "    alpha = 1.0 if name in [\"RF\",\"Surrogate\"] else 0.6\n",
    "    plt.plot(dfm[\"pt\"], dfm[\"NB\"], label=name, linestyle=ls, alpha=alpha)\n",
    "plt.axvline(0.40, ls=\":\", c=\"gray\", alpha=0.6)\n",
    "plt.xlabel(\"Soglia decisionale (pt)\")\n",
    "plt.ylabel(\"Net Benefit\")\n",
    "plt.title(\"Decision Curve ‚Äî RF vs Surrogate (test)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/decision_curve_rf_vs_surrogate.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìù Salvati:\", \n",
    "      f\"{OUT_DIR}/decision_curve_rf_sur.csv\", \n",
    "      f\"{FIG_DIR}/decision_curve_rf_vs_surrogate.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 8.c) Export regole surrogate (JSON + YAML safe)\n",
    "# =============================\n",
    "import json, math\n",
    "\n",
    "# --- salva JSON (ok con tipi numpy) ---\n",
    "json_path = f\"{OUT_DIR}/surrogate_rules_deploy.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(rules_export, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- helper ricorsivo: converti tutto in tipi Python YAML-safe ---\n",
    "def pyify(obj):\n",
    "    if obj is None:\n",
    "        return None\n",
    "    # numeri numpy -> numeri python\n",
    "    if isinstance(obj, (np.generic,)):\n",
    "        obj = obj.item()\n",
    "    # gestisci nan/inf\n",
    "    if isinstance(obj, float):\n",
    "        if math.isnan(obj) or math.isinf(obj):\n",
    "            return None\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (int, bool, str)):\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(pyify(k)): pyify(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [pyify(x) for x in obj]\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return [pyify(x) for x in obj.tolist()]\n",
    "    # fallback -> stringa\n",
    "    return str(obj)\n",
    "\n",
    "# --- salva YAML (solo se pyyaml disponibile) ---\n",
    "yaml_path = None\n",
    "if yaml is not None:\n",
    "    rules_export_yaml_safe = pyify(rules_export)\n",
    "    yaml_path = f\"{OUT_DIR}/surrogate_rules_deploy.yaml\"\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.safe_dump(\n",
    "            rules_export_yaml_safe,\n",
    "            f,\n",
    "            sort_keys=False,\n",
    "            allow_unicode=True\n",
    "        )\n",
    "\n",
    "print(\"üìù Regole esportate in:\")\n",
    "print(\" - JSON:\", json_path)\n",
    "print(\" - YAML:\", yaml_path if yaml_path else \"(pyyaml non disponibile)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 9.1 Setup & dati (se gi√† presenti, puoi saltare) ====\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
    "                             precision_recall_curve, precision_score, recall_score, f1_score)\n",
    "\n",
    "FIG_DIR = \"../figures\"; OUT_DIR = \"../data/processed\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True); os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/train_balanced_ctgan.csv\")\n",
    "feature_order = [\"MSE ML\",\"iHR V\",\"MSE V\",\"MSE AP\",\"Weigth\",\"Age\",\n",
    "                 \"Sex (M=1, F=2)\",\"H-Y\",\"Gait Speed\",\"Duration (years)\"]\n",
    "X = df[feature_order].copy()\n",
    "y = df[\"target_bin\"].astype(int).values\n",
    "\n",
    "# ==== 9.1 CV: addestramento per fold e metriche ====\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "rows, probas_folds, thr_grid = [], [], np.linspace(0.10, 0.90, 33)\n",
    "pr_curves = []  # (fold, precision, recall)\n",
    "\n",
    "for fold, (tr, va) in enumerate(kf.split(X, y), 1):\n",
    "    Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "    ytr, yva = y[tr], y[va]\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=800, min_samples_leaf=3, class_weight=\"balanced\",\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    clf.fit(Xtr, ytr)\n",
    "    p = clf.predict_proba(Xva)[:,1]\n",
    "    yhat = (p >= 0.40).astype(int)\n",
    "\n",
    "    rows.append({\n",
    "        \"fold\": fold,\n",
    "        \"ROC_AUC\": roc_auc_score(yva, p),\n",
    "        \"PR_AUC\": average_precision_score(yva, p),\n",
    "        \"Brier\": brier_score_loss(yva, p),\n",
    "        \"Precision@0.40\": precision_score(yva, yhat, zero_division=0),\n",
    "        \"Recall@0.40\": recall_score(yva, yhat, zero_division=0),\n",
    "        \"F1@0.40\": f1_score(yva, yhat, zero_division=0)\n",
    "    })\n",
    "\n",
    "    probas_folds.append({\"fold\": fold, \"p\": p, \"y\": yva})\n",
    "\n",
    "    pr = precision_recall_curve(yva, p)\n",
    "    pr_curves.append({\"fold\": fold, \"precision\": pr[0], \"recall\": pr[1]})\n",
    "\n",
    "cv_metrics = pd.DataFrame(rows)\n",
    "cv_metrics.to_csv(f\"{OUT_DIR}/cv10_metrics_rf_balanced.csv\", index=False)\n",
    "cv_summary = cv_metrics.describe().T[[\"mean\",\"std\",\"min\",\"max\"]].round(3)\n",
    "cv_summary.to_csv(f\"{OUT_DIR}/cv10_metrics_rf_balanced_summary.csv\")\n",
    "\n",
    "print(\"üìä CV10 Summary\\n\", cv_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "rf_ranks, shap_means = [], []\n",
    "\n",
    "for fold, (tr, va) in enumerate(kf.split(X, y), 1):\n",
    "    Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "    ytr, yva = y[tr], y[va]\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=800, min_samples_leaf=3, class_weight=\"balanced\",\n",
    "        random_state=fold, n_jobs=-1\n",
    "    )\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    # RF importance -> rank (1 = pi√π importante)\n",
    "    imp = pd.Series(clf.feature_importances_, index=feature_order)\n",
    "    rf_ranks.append(imp.rank(ascending=False, method=\"dense\"))\n",
    "\n",
    "    # SHAP (classe positiva, robusto a 2D/3D)\n",
    "    expl = shap.TreeExplainer(clf, data=Xtr, feature_names=feature_order)\n",
    "    sv = expl(Xva).values  # pu√≤ essere (n, f) oppure (n, 2, f) o (n, f, 2)\n",
    "\n",
    "    if sv.ndim == 3:\n",
    "        # scegli la classe positiva (asse di lunghezza 2)\n",
    "        if sv.shape[1] == 2:       # (n, 2, f)\n",
    "            sv = sv[:, 1, :]\n",
    "        elif sv.shape[2] == 2:     # (n, f, 2)\n",
    "            sv = sv[:, :, 1]\n",
    "        else:\n",
    "            raise ValueError(f\"Formato SHAP inatteso: {sv.shape}\")\n",
    "\n",
    "    # ora sv √® (n_va, n_feature)\n",
    "    mabs = pd.Series(np.abs(sv).mean(axis=0), index=feature_order)\n",
    "    shap_means.append(mabs)\n",
    "\n",
    "rf_rank_df = pd.DataFrame(rf_ranks, index=[f\"fold{f}\" for f in range(1, 11)])\n",
    "shap_mean_df = pd.DataFrame(shap_means, index=[f\"fold{f}\" for f in range(1, 11)])\n",
    "\n",
    "# aggregati e salvataggi\n",
    "rf_rank_mean = rf_rank_df.mean().sort_values()\n",
    "shap_mean_mean = shap_mean_df.mean().sort_values(ascending=False)\n",
    "shap_mean_sd = shap_mean_df.std().reindex(shap_mean_mean.index)\n",
    "\n",
    "rf_rank_df.to_csv(f\"{OUT_DIR}/cv10_rf_ranks.csv\")\n",
    "shap_mean_df.to_csv(f\"{OUT_DIR}/cv10_shap_meanabs.csv\")\n",
    "\n",
    "print(\"üèÖ RF rank medio (pi√π basso = top):\\n\", rf_rank_mean.round(2))\n",
    "print(\"\\nüî• SHAP mean|val| medio ¬± sd:\\n\",\n",
    "      pd.DataFrame({\"mean\": shap_mean_mean, \"sd\": shap_mean_sd}).round(3))\n",
    "\n",
    "# Heatmap rank RF\n",
    "plt.figure(figsize=(8.5, 5.2))\n",
    "sns.heatmap(rf_rank_df[rf_rank_mean.index], annot=False, cmap=\"crest\", cbar_kws={\"label\": \"Rank RF\"})\n",
    "plt.title(\"RF feature rank per fold (10-fold CV)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/cv10_rf_rank_heatmap.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Bar SHAP mean ¬± sd\n",
    "plt.figure(figsize=(8, 5.2))\n",
    "order = shap_mean_mean.index\n",
    "plt.barh(order, shap_mean_mean[order], xerr=shap_mean_sd[order], capsize=3, color=\"#e74c3c\", alpha=.85)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"mean(|SHAP|)  (val fold)\")\n",
    "plt.title(\"Stabilit√† SHAP ‚Äî mean ¬± sd (10-fold)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/cv10_shap_mean_bar.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Step 9.3 - Stabilit√† ranking RF vs SHAP (10-fold)\n",
    "# =====================================================\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supponiamo di avere gi√†:\n",
    "# rf_ranks: lista di ranking (len=10, ciascuno array/list di posizioni per feature_order)\n",
    "# shap_means: lista di pd.Series mean|SHAP| fold-by-fold\n",
    "\n",
    "# 1) Converti in DataFrame i rank SHAP\n",
    "shap_ranks = []\n",
    "for s in shap_means:\n",
    "    rank = s.rank(ascending=False, method=\"min\")  # 1 = pi√π importante\n",
    "    shap_ranks.append(rank.values)\n",
    "\n",
    "rf_rank_df = pd.DataFrame(rf_ranks, columns=feature_order, index=[f\"fold{f}\" for f in range(1,11)])\n",
    "shap_rank_df = pd.DataFrame(shap_ranks, columns=feature_order, index=[f\"fold{f}\" for f in range(1,11)])\n",
    "\n",
    "# 2) Calcola correlazioni Spearman tra fold\n",
    "def spearman_corr_matrix(rank_df):\n",
    "    rho = rank_df.T.corr(method=\"spearman\")\n",
    "    return rho\n",
    "\n",
    "rf_spearman = spearman_corr_matrix(rf_rank_df)\n",
    "shap_spearman = spearman_corr_matrix(shap_rank_df)\n",
    "\n",
    "# 3) Kendall‚Äôs W (concordanza globale)\n",
    "def kendall_w(rank_df):\n",
    "    \"\"\"Calcola Kendall's W di concordanza sui ranking dei fold\"\"\"\n",
    "    m = rank_df.shape[0]  # numero di fold\n",
    "    n = rank_df.shape[1]  # numero di feature\n",
    "    R = rank_df.sum(axis=0)  # somma dei rank per feature\n",
    "    mean_R = R.mean()\n",
    "    S = ((R - mean_R)**2).sum()\n",
    "    W = 12 * S / (m**2 * (n**3 - n))\n",
    "    return W\n",
    "\n",
    "rf_kendallW = kendall_w(rf_rank_df)\n",
    "shap_kendallW = kendall_w(shap_rank_df)\n",
    "\n",
    "# 4) Heatmap di similarit√† Spearman\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(rf_spearman, cmap=\"Blues\", annot=False)\n",
    "plt.title(\"Spearman RF ranks ‚Äì Similarit√† tra fold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(shap_spearman, cmap=\"Reds\", annot=False)\n",
    "plt.title(\"Spearman SHAP ranks ‚Äì Similarit√† tra fold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5) Tabella riassuntiva stabilit√†\n",
    "stab_tbl = pd.DataFrame({\n",
    "    \"Method\": [\"RF\", \"SHAP\"],\n",
    "    \"KendallW\": [rf_kendallW, shap_kendallW],\n",
    "    \"Median Spearman\": [np.median(rf_spearman.values[np.triu_indices_from(rf_spearman, k=1)]),\n",
    "                        np.median(shap_spearman.values[np.triu_indices_from(shap_spearman, k=1)])]\n",
    "})\n",
    "print(\"=== Stability across folds ===\")\n",
    "print(stab_tbl)\n",
    "\n",
    "# Salva anche in CSV\n",
    "stab_tbl.to_csv(f\"{OUT_DIR}/stability_rf_shap_cv10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# STEP 9.4 ‚Äî Consensus ranking & Stability\n",
    "# ============================\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from scipy.stats import rankdata, kendalltau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- 0) Helpers\n",
    "def _ensure_rank_frames(X, y, feature_order, n_splits=10, seed=42):\n",
    "    \"\"\"Se rf_rank_df e shap_rank_df non esistono, li ricrea in modo light.\"\"\"\n",
    "    global rf_rank_df, shap_rank_df\n",
    "\n",
    "    need_rf  = 'rf_rank_df'   not in globals()\n",
    "    need_shp = 'shap_rank_df' not in globals()\n",
    "\n",
    "    if not (need_rf or need_shp):\n",
    "        return rf_rank_df, shap_rank_df\n",
    "\n",
    "    print(\"‚ö†Ô∏è  Rank matrices not found in memory ‚Äî rebuilding (light CV)...\")\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    rf_rows   = []\n",
    "    shap_rows = []\n",
    "\n",
    "    for f, (tr, va) in enumerate(kf.split(X, y), 1):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=400, min_samples_leaf=3,\n",
    "            class_weight=\"balanced\", random_state=seed+f, n_jobs=-1\n",
    "        )\n",
    "        clf.fit(Xtr, ytr)\n",
    "\n",
    "        # --- RF ranks (bassa = importante)\n",
    "        imp = pd.Series(clf.feature_importances_, index=feature_order)\n",
    "        rf_rank = imp.rank(ascending=False, method=\"average\")\n",
    "        rf_rows.append(rf_rank.values)\n",
    "\n",
    "        # --- SHAP ranks (classe positiva)\n",
    "        expl = shap.TreeExplainer(clf, data=Xtr, feature_names=feature_order)\n",
    "        sv = expl(Xva).values  # per binario => (n_va, n_feat)\n",
    "        if sv.ndim == 3:       # alcune versioni danno (n_va, n_feat, 2)\n",
    "            sv = sv[:, :, 1]\n",
    "        mabs = np.abs(sv).mean(axis=0)\n",
    "        shp_rank = pd.Series(mabs, index=feature_order).rank(ascending=False, method=\"average\")\n",
    "        shap_rows.append(shp_rank.values)\n",
    "\n",
    "    rf_rank_df   = pd.DataFrame(rf_rows,   columns=feature_order, index=[f\"fold{f}\" for f in range(1, n_splits+1)])\n",
    "    shap_rank_df = pd.DataFrame(shap_rows, columns=feature_order, index=[f\"fold{f}\" for f in range(1, n_splits+1)])\n",
    "\n",
    "    return rf_rank_df, shap_rank_df\n",
    "\n",
    "# ---------- 1) Prepara (o ricrea) matrici di rank per fold\n",
    "rf_rank_df, shap_rank_df = _ensure_rank_frames(X_train, pd.Series(y_train), feature_order)\n",
    "\n",
    "# ---------- 2) Statistiche per metodo\n",
    "rf_mean = rf_rank_df.mean(axis=0)\n",
    "rf_sd   = rf_rank_df.std(axis=0, ddof=1)\n",
    "\n",
    "sh_mean = shap_rank_df.mean(axis=0)\n",
    "sh_sd   = shap_rank_df.std(axis=0, ddof=1)\n",
    "\n",
    "# ---------- 3) Consensus ranking (media dei rank normalizzati)\n",
    "# Normalizziamo su [1, K] (K = #feature) per entrambi e poi facciamo la media\n",
    "K = len(feature_order)\n",
    "rf_norm = (rf_mean - 1) / (K - 1)\n",
    "sh_norm = (sh_mean - 1) / (K - 1)\n",
    "consensus_score = (rf_norm + sh_norm) / 2.0\n",
    "consensus_rank  = rankdata(consensus_score, method=\"average\")  # rank crescente (pi√π basso = top)\n",
    "\n",
    "# Indice di stabilit√†: dev. std media tra RF e SHAP\n",
    "stability_sd = (rf_sd + sh_sd) / 2.0\n",
    "\n",
    "# ---------- 4) Tabella finale\n",
    "consensus_tbl = (\n",
    "    pd.DataFrame({\n",
    "        \"RF_mean_rank\":  rf_mean,\n",
    "        \"RF_sd\":         rf_sd,\n",
    "        \"SHAP_mean_rank\": sh_mean,\n",
    "        \"SHAP_sd\":        sh_sd,\n",
    "        \"Consensus_score\": consensus_score,\n",
    "        \"Consensus_rank\":  consensus_rank,\n",
    "        \"Stability_sd\":    stability_sd\n",
    "    })\n",
    "    .sort_values(\"Consensus_rank\")\n",
    ")\n",
    "\n",
    "consensus_csv = f\"{OUT_DIR}/consensus_feature_stability.csv\"\n",
    "consensus_tbl.to_csv(consensus_csv)\n",
    "print(f\"üíæ Salvato consensus table: {consensus_csv}\")\n",
    "display(consensus_tbl.style.format(precision=3))\n",
    "\n",
    "# ---------- 5) Figure 1 ‚Äî Barh consensus (rank medio ¬± sd)\n",
    "plot_df = consensus_tbl.copy().reset_index().rename(columns={\"index\":\"Feature\"})\n",
    "plot_df[\"MeanRank\"] = (plot_df[\"RF_mean_rank\"] + plot_df[\"SHAP_mean_rank\"]) / 2.0\n",
    "\n",
    "plt.figure(figsize=(8.8, 6))\n",
    "order = plot_df.sort_values(\"Consensus_rank\")[\"Feature\"]\n",
    "y = np.arange(len(order))\n",
    "\n",
    "plt.barh(y, plot_df.set_index(\"Feature\").loc[order, \"MeanRank\"], \n",
    "         xerr=plot_df.set_index(\"Feature\").loc[order, \"Stability_sd\"],\n",
    "         capsize=4, color=\"#e74c3c\", alpha=0.85)\n",
    "plt.yticks(y, order)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Rank medio (pi√π basso = pi√π importante)\")\n",
    "plt.title(\"Consensus ranking ‚Äî mean rank ¬± sd (RF & SHAP, 10-fold)\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "f1 = f\"{FIG_DIR}/consensus_rank_barh.png\"\n",
    "plt.savefig(f1, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------- 6) Figure 2 ‚Äî RF vs SHAP mean ranks (concordanza)\n",
    "plt.figure(figsize=(6.8, 5.6))\n",
    "plt.scatter(rf_mean, sh_mean, s=70, alpha=0.8, color=\"#7b4397\")\n",
    "for f in feature_order:\n",
    "    plt.text(rf_mean[f]+0.05, sh_mean[f]+0.05, f, fontsize=9)\n",
    "lims = [0.8, K+0.2]\n",
    "plt.plot(lims, lims, \"--\", color=\"gray\", alpha=0.5)\n",
    "plt.xlim(lims); plt.ylim(lims)\n",
    "plt.xlabel(\"RF mean rank\")\n",
    "plt.ylabel(\"SHAP mean rank\")\n",
    "rho, _ = kendalltau(rf_mean, sh_mean)\n",
    "plt.title(f\"Concordanza RF vs SHAP (mean rank) ‚Äî Kendall œÑ = {rho:.2f}\")\n",
    "plt.tight_layout()\n",
    "f2 = f\"{FIG_DIR}/rf_vs_shap_meanrank_scatter.png\"\n",
    "plt.savefig(f2, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------- 7) Figure 3 ‚Äî Heatmap delta rank (SHAP - RF)\n",
    "delta = (sh_mean - rf_mean).to_frame(\"Œîrank (SHAP - RF)\").T\n",
    "plt.figure(figsize=(8.8, 2.6))\n",
    "sns.heatmap(delta, cmap=\"coolwarm\", center=0, annot=True, fmt=\".2f\",\n",
    "            cbar_kws={\"label\":\"Œîrank (SHAP - RF)\"})\n",
    "plt.title(\"Consensus heatmap ‚Äî differenza media di rank\")\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "f3 = f\"{FIG_DIR}/consensus_heatmap_delta.png\"\n",
    "plt.savefig(f3, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------- 8) Riassunto finale per il paper (Top-5)\n",
    "top5 = consensus_tbl.sort_values(\"Consensus_rank\").head(5).copy()\n",
    "print(\"üèÜ Top-5 consensus features:\")\n",
    "display(top5[[\"RF_mean_rank\",\"SHAP_mean_rank\",\"Consensus_rank\",\"Stability_sd\"]].style.format(precision=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= STEP 10 ‚Äî Figure semplici e pulite =========\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import shap, os\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "FIG_DIR = \"../figures\"; os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# --- Helper per gestire differenze di versione SHAP (sempre classe positiva) ----\n",
    "def ensure_1d(a, n_expected=None, take_last_axis=True):\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim == 0:\n",
    "        a = a.reshape(1)\n",
    "    if a.ndim == 2:\n",
    "        # Es.: (n, 2) -> prendo la seconda colonna (classe 1)\n",
    "        a = a[:, -1] if take_last_axis else a[:, 0]\n",
    "    if (n_expected is not None) and (len(a) != n_expected):\n",
    "        # in estrema difesa: taglio/pad\n",
    "        a = a[:n_expected]\n",
    "    return a\n",
    "\n",
    "# --- SHAP explainer + proba su test (se mancano) ---\n",
    "try:\n",
    "    shap_expl\n",
    "except NameError:\n",
    "    explainer = shap.TreeExplainer(rf, data=X_train, feature_names=feature_order)\n",
    "    shap_expl = explainer(X_test)\n",
    "\n",
    "try:\n",
    "    p_test\n",
    "except NameError:\n",
    "    p_test = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "n_test = len(X_test)\n",
    "\n",
    "# =============== 10.1 SHAP-dependence (MSE ML, color=iHR V) ====================\n",
    "feat_x = \"MSE ML\"\n",
    "feat_c = \"iHR V\"\n",
    "\n",
    "# x: valore della feature sul test\n",
    "x_vals = X_test[feat_x].to_numpy()\n",
    "\n",
    "# y: shap value della stessa feature (classe positiva)\n",
    "# Per massima compatibilit√† uso sia lo slicing \"nuovo\" sia quello \"vecchio\"\n",
    "try:\n",
    "    y_vals = shap_expl[:, feat_x].values\n",
    "except Exception:\n",
    "    col = X_test.columns.get_loc(feat_x)\n",
    "    y_vals = shap_expl.values[:, col]\n",
    "y_vals = ensure_1d(y_vals, n_expected=n_test)\n",
    "\n",
    "# colore: valore (raw) della feature di colore\n",
    "try:\n",
    "    c_vals = shap_expl[:, feat_c].data\n",
    "except Exception:\n",
    "    c_vals = X_test[feat_c].to_numpy()\n",
    "c_vals = ensure_1d(c_vals, n_expected=n_test, take_last_axis=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4.2))\n",
    "sc = plt.scatter(x_vals, y_vals, c=c_vals, s=38, cmap=\"magma\", edgecolor=\"none\")\n",
    "plt.xlabel(feat_x); plt.ylabel(f\"SHAP value for {feat_x}\")\n",
    "cb = plt.colorbar(sc); cb.set_label(feat_c)\n",
    "plt.title(f\"SHAP dependence ‚Äî {feat_x} (color: {feat_c})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/step10a_shap_dependence_{feat_x.replace(' ','_')}.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ================== 10.2 PDP 2D (MSE ML √ó MSE V) ===============================\n",
    "f1, f2 = \"MSE ML\", \"MSE V\"\n",
    "i1, i2 = X_test.columns.get_loc(f1), X_test.columns.get_loc(f2)\n",
    "pd2 = partial_dependence(rf, X_test, [(i1, i2)], grid_resolution=25,\n",
    "                         kind=\"average\", method=\"brute\")\n",
    "\n",
    "if isinstance(pd2, dict):   # sklearn >= 1.4\n",
    "    surf = pd2[\"average\"][0]\n",
    "    g1, g2 = pd2[\"values\"]\n",
    "else:                        # sklearn <= 1.3\n",
    "    surf = pd2.average[0]\n",
    "    g1, g2 = pd2.grid_values\n",
    "\n",
    "plt.figure(figsize=(7.2, 4.6))\n",
    "sns.heatmap(surf, xticklabels=np.round(g2, 2), yticklabels=np.round(g1, 2),\n",
    "            cmap=\"magma\", cbar_kws={\"label\": \"Partial dependence P(Triade)\"})\n",
    "plt.xlabel(f2); plt.ylabel(f1)\n",
    "plt.title(f\"PDP 2D ‚Äî {f1} √ó {f2} (test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/step10b_pdp2d_{f1.replace(' ','_')}_x_{f2.replace(' ','_')}.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ================== 10.3 Decision Curve Analysis (semplice) ====================\n",
    "def decision_curve(y_true, p, pt_grid=None):\n",
    "    if pt_grid is None:\n",
    "        pt_grid = np.linspace(0.05, 0.9, 40)\n",
    "    nb_model, nb_all = [], []\n",
    "    Y = y_true.astype(int)\n",
    "    for pt in pt_grid:\n",
    "        pred = (p >= pt).astype(int)\n",
    "        TP = np.sum((pred == 1) & (Y == 1))\n",
    "        FP = np.sum((pred == 1) & (Y == 0))\n",
    "        N  = len(Y)\n",
    "        nb = (TP/N) - (FP/N) * (pt/(1-pt))\n",
    "        nb_model.append(nb)\n",
    "        # treat-all\n",
    "        prev = Y.mean()\n",
    "        nb_all.append(prev - (1 - prev) * (pt/(1-pt)))\n",
    "    return pd.DataFrame({\"pt\": pt_grid, \"NB_model\": nb_model, \"NB_all\": nb_all})\n",
    "\n",
    "dca = decision_curve(y_test, p_test)\n",
    "plt.figure(figsize=(7.2, 4.6))\n",
    "plt.plot(dca[\"pt\"], dca[\"NB_model\"], label=\"RF\")\n",
    "plt.plot(dca[\"pt\"], dca[\"NB_all\"], \"--\", label=\"Tratta tutti\", alpha=0.7)\n",
    "plt.axhline(0, ls=\"--\", color=\"grey\", label=\"Tratta nessuno\", alpha=0.7)\n",
    "plt.axvline(0.40, ls=\":\", color=\"grey\", alpha=0.7)\n",
    "plt.xlabel(\"Soglia decisionale (pt)\"); plt.ylabel(\"Net Benefit\")\n",
    "plt.title(\"Decision Curve ‚Äî RF (test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/step10c_decision_curve_rf.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ================== 10.4 Calibration (decile plot) =============================\n",
    "def calibration_by_decile(y_true, proba, bins=10):\n",
    "    df = pd.DataFrame({\"y\": y_true, \"p\": proba}).sort_values(\"p\")\n",
    "    df[\"bin\"] = pd.qcut(df[\"p\"], q=bins, duplicates=\"drop\")\n",
    "    grp = df.groupby(\"bin\", observed=False)\n",
    "    out = grp.agg(\n",
    "        pred=(\"p\", \"mean\"),\n",
    "        obs=(\"y\", \"mean\"),\n",
    "        n=(\"y\", \"size\")\n",
    "    ).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "cal = calibration_by_decile(y_test, p_test, bins=10)\n",
    "plt.figure(figsize=(6.0, 4.8))\n",
    "plt.plot([0,1], [0,1], \"--\", color=\"grey\", label=\"Perfectly calibrated\")\n",
    "plt.plot(cal[\"pred\"], cal[\"obs\"], \"o-\", label=\"Observed\")\n",
    "for i, n in enumerate(cal[\"n\"]):\n",
    "    plt.annotate(f\"n={int(n)}\", (cal[\"pred\"][i], cal[\"obs\"][i]),\n",
    "                 textcoords=\"offset points\", xytext=(2,5), fontsize=8)\n",
    "plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Observed prevalence\")\n",
    "plt.title(\"Calibration (decile bins) ‚Äî test\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/step10d_calibration_decile.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# (opzionale) report di salvataggio\n",
    "print(\"‚úîÔ∏è  Figure salvate in:\", FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STEP 10 ‚Äî Synthesis & Paper-ready Assets\n",
    "# =========================================\n",
    "import os, json, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
    "                             precision_recall_curve, roc_curve, precision_score,\n",
    "                             recall_score, f1_score)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.inspection import partial_dependence\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "import shap\n",
    "\n",
    "# ---------- Paths\n",
    "FIG_DIR = \"../figurez/global\"\n",
    "OUT_DIR = \"../tables\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Data & features\n",
    "train = pd.read_csv(\"../data/processed/train_balanced_ctgan.csv\")\n",
    "test  = pd.read_csv(\"../data/processed/test_original.csv\")\n",
    "\n",
    "feature_order = [\n",
    "    \"MSE ML\",\"iHR V\",\"MSE V\",\"MSE AP\",\"Weigth\",\"Age\",\n",
    "    \"Sex (M=1, F=2)\",\"H-Y\",\"Gait Speed\",\"Duration (years)\"\n",
    "]\n",
    "X_train = train[feature_order].copy()\n",
    "y_train = train[\"target_bin\"].astype(int).values\n",
    "X_test  = test[feature_order].copy()\n",
    "y_test  = test[\"target_bin\"].astype(int).values\n",
    "\n",
    "# ---------- Final RF (stesso setup usato negli step precedenti)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    min_samples_leaf=3,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "p_test = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# ---------- Metriche globali + soglie cliniche\n",
    "def metrics_global(y, p):\n",
    "    return dict(\n",
    "        ROC_AUC=float(roc_auc_score(y, p)),\n",
    "        PR_AUC=float(average_precision_score(y, p)),\n",
    "        Brier=float(brier_score_loss(y, p))\n",
    "    )\n",
    "\n",
    "def metrics_at_threshold(y, p, thr):\n",
    "    pred = (p >= thr).astype(int)\n",
    "    return dict(\n",
    "        thr=float(thr),\n",
    "        precision=float(precision_score(y, pred, zero_division=0)),\n",
    "        recall=float(recall_score(y, pred, zero_division=0)),\n",
    "        f1=float(f1_score(y, pred, zero_division=0)),\n",
    "        specificity=float(((pred==0) & (y==0)).sum() / (y==0).sum()),\n",
    "        brier=float(brier_score_loss(y, p))\n",
    "    )\n",
    "\n",
    "glob = metrics_global(y_test, p_test)\n",
    "thr_tbl = pd.DataFrame([metrics_at_threshold(y_test, p_test, t) for t in [0.26, 0.40, 0.50]])\n",
    "thr_tbl.to_csv(f\"{OUT_DIR}/step10_threshold_metrics.csv\", index=False)\n",
    "\n",
    "print(\"Test metrics:\", glob)\n",
    "print(\"\\nMetrics @ thresholds:\\n\", thr_tbl)\n",
    "\n",
    "# ---------- Decision Curve Analysis (Net Benefit)\n",
    "def decision_curve_net_benefit(y, p, pts=np.linspace(0.05,0.90,40)):\n",
    "    y = np.asarray(y); p = np.asarray(p)\n",
    "    N = len(y)\n",
    "    out = []\n",
    "    for pt in pts:\n",
    "        pred = (p >= pt).astype(int)\n",
    "        TP = np.sum((pred==1) & (y==1))\n",
    "        FP = np.sum((pred==1) & (y==0))\n",
    "        NB = (TP/N) - (FP/N) * (pt/(1-pt))\n",
    "        # strategie baseline\n",
    "        prev = y.mean()\n",
    "        NB_all  = prev - (1-prev) * (pt/(1-pt))\n",
    "        NB_none = 0.0\n",
    "        out.append((pt, NB, NB_all, NB_none))\n",
    "    return pd.DataFrame(out, columns=[\"pt\",\"NB_model\",\"NB_all\",\"NB_none\"])\n",
    "\n",
    "dca = decision_curve_net_benefit(y_test, p_test)\n",
    "dca.to_csv(f\"{OUT_DIR}/step10_decision_curve.csv\", index=False)\n",
    "\n",
    "# ---------- SHAP (classe positiva) per plots di sintesi\n",
    "expl = shap.TreeExplainer(rf, data=X_train, feature_names=feature_order)\n",
    "sh_expl = expl(X_test)  # shap.Explanation\n",
    "# se multi-output, seleziona classe 1\n",
    "if hasattr(sh_expl.values, \"ndim\") and sh_expl.values.ndim == 3:\n",
    "    vals = sh_expl.values[:,:,1]\n",
    "    bases = sh_expl.base_values[:,1] if np.ndim(sh_expl.base_values)==2 else sh_expl.base_values\n",
    "    shap_pos = shap.Explanation(values=vals, base_values=bases,\n",
    "                                data=X_test.values, feature_names=feature_order)\n",
    "else:\n",
    "    shap_pos = sh_expl\n",
    "\n",
    "# ---------- Consensus ranking (RF + SHAP) via 10-fold (veloce ma riproducibile)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "rf_ranks, shap_ranks = [], []\n",
    "\n",
    "for tr, va in kf.split(X_train, y_train):\n",
    "    Xtr, Xva = X_train.iloc[tr], X_train.iloc[va]\n",
    "    ytr, yva = y_train[tr], y_train[va]\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=400, min_samples_leaf=3,\n",
    "        class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "    ).fit(Xtr, ytr)\n",
    "\n",
    "    # RF rank\n",
    "    rf_imp = pd.Series(clf.feature_importances_, index=feature_order)\n",
    "    rf_ranks.append(rf_imp.rank(ascending=False, method=\"average\"))\n",
    "\n",
    "    # SHAP rank\n",
    "    ex = shap.TreeExplainer(clf, data=Xtr, feature_names=feature_order)\n",
    "    sv = ex(Xva).values\n",
    "    if sv.ndim == 3: sv = sv[:,:,1]\n",
    "    mabs = pd.Series(np.abs(sv).mean(axis=0), index=feature_order)\n",
    "    shap_ranks.append(mabs.rank(ascending=False, method=\"average\"))\n",
    "\n",
    "rf_rank_df   = pd.DataFrame(rf_ranks,   columns=feature_order)\n",
    "shap_rank_df = pd.DataFrame(shap_ranks, columns=feature_order)\n",
    "\n",
    "rf_mean   = rf_rank_df.mean(0);   rf_sd   = rf_rank_df.std(0, ddof=1)\n",
    "sh_mean   = shap_rank_df.mean(0); sh_sd   = shap_rank_df.std(0, ddof=1)\n",
    "\n",
    "consensus = pd.DataFrame({\n",
    "    \"RF_mean_rank\": rf_mean, \"RF_sd\": rf_sd,\n",
    "    \"SHAP_mean_rank\": sh_mean, \"SHAP_sd\": sh_sd\n",
    "})\n",
    "consensus[\"Consensus_score\"] = (consensus[\"RF_mean_rank\"] + consensus[\"SHAP_mean_rank\"])/20.0\n",
    "consensus[\"Consensus_rank\"]  = consensus[\"Consensus_score\"].rank(method=\"min\")\n",
    "consensus[\"Stability_sd\"]    = 0.5*(consensus[\"RF_sd\"] + consensus[\"SHAP_sd\"])\n",
    "consensus = consensus.sort_values(\"Consensus_rank\")\n",
    "consensus.to_csv(f\"{OUT_DIR}/step10_consensus_ranking.csv\")\n",
    "\n",
    "tau, _ = kendalltau(rf_mean, sh_mean)\n",
    "print(f\"\\nKendall œÑ (RF vs SHAP mean rank): {tau:.2f}\")\n",
    "\n",
    "# ---------- PDP 2D (MSE ML √ó MSE V) per pannello\n",
    "def pdp2d(model, X, a, b, grid=25):\n",
    "    pa = partial_dependence(model, X, [(X.columns.get_loc(a), X.columns.get_loc(b))],\n",
    "                            grid_resolution=grid, kind=\"average\", method=\"brute\")\n",
    "    if isinstance(pa, dict):\n",
    "        surf = pa[\"average\"][0]; g1, g2 = pa[\"values\"]\n",
    "    else:\n",
    "        surf = pa.average[0]; g1, g2 = pa.grid_values\n",
    "    return g1, g2, surf\n",
    "\n",
    "g1, g2, surf = pdp2d(rf, X_test, \"MSE ML\", \"MSE V\", grid=21)\n",
    "\n",
    "# ---------- FIGURE OVERVIEW (4 pannelli)\n",
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "# A) Consensus barh\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ord_idx = consensus.index\n",
    "ax1.barh(ord_idx, consensus.loc[ord_idx,\"RF_mean_rank\"], height=0.35, label=\"RF\", color=\"#6c5ce7\")\n",
    "ax1.barh(ord_idx, consensus.loc[ord_idx,\"SHAP_mean_rank\"], height=0.35, left=0, alpha=0.45, label=\"SHAP\", color=\"#e84393\")\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_title(\"Consensus ranking ‚Äî mean rank (RF & SHAP, 10-fold)\")\n",
    "ax1.set_xlabel(\"Rank (pi√π basso = pi√π importante)\")\n",
    "ax1.legend(frameon=False)\n",
    "\n",
    "# B) SHAP dependence MSE ML (color: iHR V)\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "shap.plots.scatter(shap_pos[:, \"MSE ML\"], color=shap_pos[:, \"iHR V\"], show=False)\n",
    "ax2.set_title(\"SHAP dependence ‚Äî MSE ML (color: iHR V)\")\n",
    "\n",
    "# C) PDP 2D heatmap\n",
    "ax3 = plt.subplot(2,2,3)\n",
    "sns.heatmap(surf, xticklabels=np.round(g2,2), yticklabels=np.round(g1,2),\n",
    "            cmap=\"magma\", cbar_kws={\"label\":\"Partial dependence P(Triade)\"}, ax=ax3)\n",
    "ax3.set_xlabel(\"MSE V\"); ax3.set_ylabel(\"MSE ML\")\n",
    "ax3.set_title(\"PDP 2D ‚Äî MSE ML √ó MSE V\")\n",
    "\n",
    "# D) Decision Curve\n",
    "ax4 = plt.subplot(2,2,4)\n",
    "ax4.plot(dca[\"pt\"], dca[\"NB_model\"], label=\"RF\", lw=2)\n",
    "ax4.plot(dca[\"pt\"], dca[\"NB_none\"], \"--\", color=\"gray\", label=\"Tratta nessuno\")\n",
    "ax4.plot(dca[\"pt\"], dca[\"NB_all\"],  \"--\", color=\"#c0392b\", alpha=0.6, label=\"Tratta tutti\")\n",
    "ax4.axvline(0.40, ls=\":\", color=\"gray\", alpha=0.7)\n",
    "ax4.set_xlabel(\"Soglia decisionale (pt)\"); ax4.set_ylabel(\"Net Benefit\")\n",
    "ax4.set_title(\"Decision Curve ‚Äî RF (test)\")\n",
    "ax4.legend(frameon=False, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/step10_overview_panel.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------- Calibration: reliability plot + histogram\n",
    "def calibration_plot(y, p, nbins=10, fname=None):\n",
    "    df = pd.DataFrame({\"y\":y, \"p\":p})\n",
    "    df[\"bin\"] = pd.qcut(df[\"p\"], q=nbins, duplicates=\"drop\")\n",
    "    grp = df.groupby(\"bin\").agg(p_mean=(\"p\",\"mean\"), y_rate=(\"y\",\"mean\"), n=(\"y\",\"size\")).reset_index(drop=True)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot([0,1],[0,1],\"--\",color=\"gray\",alpha=0.6,label=\"Perfectly calibrated\")\n",
    "    plt.plot(grp[\"p_mean\"], grp[\"y_rate\"], marker=\"o\", lw=2, label=\"Observed\")\n",
    "    for _,r in grp.iterrows():\n",
    "        plt.text(r[\"p_mean\"], r[\"y_rate\"]+0.015, f\"n={int(r['n'])}\", ha=\"center\", fontsize=8)\n",
    "    plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Observed prevalence\")\n",
    "    plt.title(\"Calibration (decile bins) ‚Äî test\")\n",
    "    plt.grid(alpha=0.2); plt.legend(frameon=False)\n",
    "    if fname:\n",
    "        plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "calibration_plot(y_test, p_test, nbins=10, fname=f\"{FIG_DIR}/step10_calibration.png\")\n",
    "\n",
    "# ---------- Model Card (paper-ready mini profile)\n",
    "top5 = consensus.sort_values(\"Consensus_rank\").head(5).index.tolist()\n",
    "model_card = {\n",
    "    \"dataset\": {\n",
    "        \"train_balanced_ctgan_shape\": list(X_train.shape),\n",
    "        \"test_original_shape\": list(X_test.shape),\n",
    "        \"test_target_dist\": dict(pd.Series(y_test).value_counts().sort_index())\n",
    "    },\n",
    "    \"model\": \"RandomForestClassifier (n_estimators=800, min_samples_leaf=3, class_weight='balanced')\",\n",
    "    \"test_metrics\": glob,\n",
    "    \"threshold_metrics\": thr_tbl.to_dict(orient=\"records\"),\n",
    "    \"consensus_top5_features\": top5,\n",
    "    \"kendall_tau_RF_vs_SHAP\": float(tau),\n",
    "    \"figures\": {\n",
    "        \"overview_panel\": f\"{FIG_DIR}/step10_overview_panel.png\",\n",
    "        \"calibration\": f\"{FIG_DIR}/step10_calibration.png\"\n",
    "    }\n",
    "}\n",
    "with open(f\"{OUT_DIR}/step10_model_card.json\",\"w\") as f: json.dump(model_card, f, indent=2)\n",
    "\n",
    "md = f\"\"\"# Model Card ‚Äî Triade RF (Explainability Synthesis)\n",
    "\n",
    "**Train/Test**: train_balanced_ctgan={X_train.shape}, test_original={X_test.shape}  \n",
    "**Test metrics**: ROC_AUC={glob['ROC_AUC']:.3f}, PR_AUC={glob['PR_AUC']:.3f}, Brier={glob['Brier']:.3f}\n",
    "\n",
    "**Threshold metrics** (test):\n",
    "{thr_tbl.round(3).to_markdown(index=False)}\n",
    "\n",
    "**Consensus Top-5 (RF+SHAP, 10-fold)**: {', '.join(top5)}\n",
    "\n",
    "**RF vs SHAP rank concordance**: Kendall œÑ = {tau:.2f}\n",
    "\n",
    "**Figures**:\n",
    "- `step10_overview_panel.png` (consensus, SHAP dependence, PDP 2D, DCA)\n",
    "- `step10_calibration.png` (reliability)\n",
    "\"\"\"\n",
    "with open(f\"{OUT_DIR}/step10_model_card.md\",\"w\") as f: f.write(md)\n",
    "\n",
    "print(\"‚úÖ Saved:\",\n",
    "      f\"\\n- {FIG_DIR}/step10_overview_panel.png\",\n",
    "      f\"\\n- {FIG_DIR}/step10_calibration.png\",\n",
    "      f\"\\n- {OUT_DIR}/step10_model_card.json\",\n",
    "      f\"\\n- {OUT_DIR}/step10_model_card.md\",\n",
    "      f\"\\n- {OUT_DIR}/step10_consensus_ranking.csv\",\n",
    "      f\"\\n- {OUT_DIR}/step10_threshold_metrics.csv\",\n",
    "      f\"\\n- {OUT_DIR}/step10_decision_curve.csv\",\n",
    ")\n",
    "\n",
    "# ---------- Patient-level report utility (1-liner + SHAP waterfall)\n",
    "def make_patient_report(idx, thr=0.40, save_dir=FIG_DIR):\n",
    "    \"\"\"Crea un mini-report per un soggetto del test set.\"\"\"\n",
    "    assert 0 <= idx < len(X_test), \"idx fuori range (test set)\"\n",
    "    proba = float(p_test[idx]); y = int(y_test[idx])\n",
    "    pred = int(proba >= thr)\n",
    "    # Waterfall SHAP (classe positiva)\n",
    "    ex = shap_pos[idx]\n",
    "    plt.figure(figsize=(7.2,5.2))\n",
    "    shap.plots.waterfall(ex, max_display=12, show=False)\n",
    "    title = f\"Waterfall ‚Äî id={idx} | y={y} | p={proba:.2f} | thr={thr} ‚Üí pred={pred}\"\n",
    "    plt.title(title); plt.tight_layout()\n",
    "    fn = f\"{save_dir}/step10_local_waterfall_id{idx}.png\"\n",
    "    plt.savefig(fn, dpi=300); plt.show()\n",
    "    # Riepilogo riga\n",
    "    row = X_test.iloc[idx].to_dict()\n",
    "    print(\"‚ñ∂Ô∏è  Patient report\",\n",
    "          f\"\\n id={idx}  y={y}  p={proba:.3f}  pred@{thr}={pred}\",\n",
    "          f\"\\n key features:\", {k: round(row[k],3) for k in ['MSE ML','MSE V','iHR V','Gait Speed','Weigth']},\n",
    "          f\"\\n file: {fn}\"\n",
    "         )\n",
    "    return fn\n",
    "\n",
    "# Esempio: genera un report per il soggetto con p pi√π vicino alla soglia 0.40\n",
    "closest_idx = int(np.argsort(np.abs(p_test - 0.40))[0])\n",
    "_ = make_patient_report(closest_idx, thr=0.40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
